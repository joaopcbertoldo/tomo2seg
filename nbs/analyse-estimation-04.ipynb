{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import functools\n",
    "import gc\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import PrettyPrinter, pprint\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanize\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from matplotlib import cm, patches, pyplot as plt\n",
    "from numpy import ndarray\n",
    "from numpy.random import RandomState\n",
    "from progressbar import progressbar as pbar\n",
    "from pymicro.file import file_utils\n",
    "from sklearn import metrics, metrics as met, model_selection, preprocessing\n",
    "from skimage import measure as skimage_measure\n",
    "import tabulate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import (\n",
    "    callbacks as keras_callbacks,\n",
    "    layers,\n",
    "    losses,\n",
    "    metrics as keras_metrics,\n",
    "    optimizers,\n",
    "    utils,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from yaml import YAMLObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from tomo2seg import (\n",
    "    analyse as t2s_analyse,\n",
    "    callbacks as tomo2seg_callbacks,\n",
    "    data as tomo2seg_data,\n",
    "    hosts,\n",
    "    losses as tomo2seg_losses,\n",
    "    schedule as tomo2seg_schedule,\n",
    "    slack,\n",
    "    slackme,\n",
    "    utils as tomo2seg_utils,\n",
    "    viz as t2s_viz,\n",
    "    volume_sequence,\n",
    ")\n",
    "from tomo2seg.data import EstimationVolume, Volume\n",
    "from tomo2seg.logger import add_file_handler, dict2str, logger\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg.analyse_pred import AnalysePredMetaArgs as MetaArgs\n",
    "from tomo2seg.analyse_pred import AnalysePredOuputs as Outputs\n",
    "from tomo2seg.analyse_pred import AnalysePredOpts as Opts\n",
    "from tomo2seg import hosts as t2s_hosts\n",
    "from tomo2seg import datasets as t2s_datasets\n",
    "from tomo2seg import analyse_gt, analyse_pred, analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "meta_args = MetaArgs(\n",
    "    \n",
    "    script_name = \"analyse-estimation-04.ipynb\",\n",
    "    \n",
    "    volume_name = t2s_datasets.VOLUME_COMPOSITE_V1[0],\n",
    "    volume_version = t2s_datasets.VOLUME_COMPOSITE_V1[1],\n",
    "    labels_version = t2s_datasets.VOLUME_COMPOSITE_V1_LABELS_REFINED3,\n",
    "    \n",
    "#     estimation_volume_fullname = sys.argv[1],\n",
    "    estimation_volume_fullname = (\n",
    "        \"vol=PA66GF30.v1.set=test.model=paper-unet-2d.full-f16.fold000.1611-743-205.runid=1611-789-693\"\n",
    "    ),\n",
    "    \n",
    "    opts = Opts(\n",
    "        compute = Opts.Compute(\n",
    "            error_volume = True,\n",
    "\n",
    "            roc_curve = False,\n",
    "            multiclass_roc_auc = False,\n",
    "\n",
    "            error_blobs_2d_props = True,\n",
    "            error_blobs_3d_props = False,\n",
    "\n",
    "            adjacent_layers_correlation = True,\n",
    "        ),\n",
    "        save = Opts.Save(\n",
    "            confusion_volume = False,\n",
    "            error_volume = True,\n",
    "            \n",
    "            error_blobs_2d_props = False,\n",
    "            error_blobs_3d_props = False\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "    host=None,  # None = auto\n",
    "    runid=None,  # None = auto\n",
    "    random_state_seed=42,  # None = auto\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tomo2seg` objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Volume.with_check(\n",
    "    name=meta_args.volume_name, \n",
    "    version=meta_args.volume_version,\n",
    ")\n",
    "\n",
    "estimation_volume = EstimationVolume.from_fullname(\n",
    "    meta_args.estimation_volume_fullname,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert estimation_volume.volume_fullname == volume.fullname\n",
    "\n",
    "if estimation_volume.partition is None:\n",
    "    raise NotImplementedError(f\"{estimation_volume.partition=}\")\n",
    "    \n",
    "# todo replace me by a similar structure inside the outputs object\n",
    "estimation_volume[\"analyse-pred-meta_args\"] = asdict(meta_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filepath = estimation_volume.analyse_exec_log_path\n",
    "\n",
    "random_state_seed = meta_args.random_state_seed\n",
    "random_state = np.random.RandomState(random_state_seed)\n",
    "\n",
    "runid = meta_args.runid\n",
    "\n",
    "script_name = meta_args.script_name\n",
    "hostname = meta_args.host.hostname\n",
    "\n",
    "opts = copy.deepcopy(meta_args.opts)\n",
    "\n",
    "data_path = str(volume.data_path)\n",
    "data_meta = dict(\n",
    "    dtype = volume.metadata.dtype,\n",
    "    dims = volume.metadata.dimensions,\n",
    ")\n",
    "\n",
    "labels_path = str(volume.versioned_labels_path(meta_args.labels_version))\n",
    "preds_path = str(estimation_volume.predictions_path)\n",
    "probas_path = str(estimation_volume.probabilities_path)\n",
    "\n",
    "partition = estimation_volume.partition\n",
    "partition_dims = (\n",
    "    estimation_volume.partition.shape \n",
    "    if estimation_volume.partition is not None else \n",
    "    volume.metadata.dimensions\n",
    ")\n",
    "partition_slice = analyse_gt.partition2slice(partition) \n",
    "\n",
    "# float16 instead of 64 to save memory\n",
    "proba_dtype = np.float16\n",
    "\n",
    "cv_dtype = np.int16\n",
    "\n",
    "labels_idx = volume.metadata.labels\n",
    "labels_names = [volume.metadata.labels_names[idx] for idx in labels_idx]\n",
    "labels_idx_name = dict(zip(labels_idx, labels_names))\n",
    "n_classes = len(labels_idx)\n",
    "\n",
    "# parallel_nprocs = host.analyse_parallel_nprocs\n",
    "parallel_nprocs = None  # use it all\n",
    "\n",
    "# todo move me to the estim volume obs\n",
    "outputs_dir = estimation_volume.dir / \"pred-analysis\"  \n",
    "outputs_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = Outputs(outputs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "add_file_handler(logger, log_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"{volume.__class__.__name__}{dict2str(asdict(volume))}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"{estimation_volume.__class__.__name__}{dict2str(asdict(estimation_volume))}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"{opts.__class__.__name__}{dict2str(asdict(opts))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading data from disk.\")\n",
    "data_volume = file_utils.HST_read(\n",
    "    data_path,  # it doesn't accept paths...\n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=data_meta[\"dtype\"],\n",
    "    dims=data_meta[\"dims\"],\n",
    "    verbose=False,\n",
    ")[partition_slice]\n",
    "logger.debug(f\"{data_volume.shape=}\")\n",
    "\n",
    "logger.info(\"Loading labels from disk.\")\n",
    "labels_volume = file_utils.HST_read(\n",
    "    labels_path,  # it doesn't accept paths...\n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=\"uint8\",\n",
    "    dims=data_meta[\"dims\"],\n",
    "    verbose=False,\n",
    ")[partition_slice]\n",
    "logger.debug(f\"{labels_volume.shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading predictions from disk.\")\n",
    "preds_volume = file_utils.HST_read(\n",
    "    preds_path,  # it doesn't accept paths...\n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=\"uint8\",\n",
    "    dims=partition_dims,\n",
    "    verbose=False,\n",
    ")\n",
    "logger.debug(f\"{preds_volume.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading probabilities from disk.\")\n",
    "probas_volume = np.load(probas_path).astype(proba_dtype)\n",
    "logger.debug(f\"{probas_volume.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [compute] confusion volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Computing confusion volume.\")\n",
    "\n",
    "cv_encoding, cv_encoding_inv = analyse_pred.get_conf_vol_encoding(labels_idx)\n",
    "\n",
    "logger.debug(f\"cv_encoding\\n{dict2str(cv_encoding)}\")\n",
    "estimation_volume[\"cv_encoding\"] = cv_encoding\n",
    "\n",
    "logger.debug(f\"cv_encoding_inv\\n{dict2str(cv_encoding_inv)}\")\n",
    "estimation_volume[\"cv_encoding_inv\"] = cv_encoding_inv\n",
    "\n",
    "# 10000 is an impossible encoding\n",
    "conf_vol = np.full_like(labels_volume, 10000, dtype=cv_dtype)\n",
    "\n",
    "for (gt_idx, pred_idx), encoded_value in pbar(\n",
    "    cv_encoding.items(),\n",
    "    max_value=len(cv_encoding)\n",
    "):\n",
    "    conf_vol[\n",
    "        (labels_volume == gt_idx) & (preds_volume == pred_idx)\n",
    "    ] = encoded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(conf_vol != 10000), \"10000 is an impossible encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [save] confusion volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving confusion volume.\")\n",
    "\n",
    "if opts.save.confusion_volume:\n",
    "    file_utils.HST_write(conf_vol, str(outputs.confusion_volume))   \n",
    "    logger.info(\"done\")\n",
    "else: \n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [compute] error volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing error volume.\")\n",
    "\n",
    "if opts.compute.error_volume:\n",
    "\n",
    "    error_volume = np.full_like(labels_volume, False, dtype=bool)\n",
    "\n",
    "    for label_idx in pbar(labels_idx):\n",
    "\n",
    "        encoded_value = cv_encoding[(label_idx, label_idx)]\n",
    "        \n",
    "        error_volume |= (conf_vol == encoded_value)\n",
    "\n",
    "    error_volume = ~error_volume\n",
    "    \n",
    "    logger.info(\"done\")\n",
    "\n",
    "else: \n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [save] error volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving error volume.\")\n",
    "\n",
    "if opts.save.error_volume:\n",
    "    file_utils.HST_write(error_volume, str(outputs.error_volume_path))    \n",
    "    logger.info(\"done\")\n",
    "else:\n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [compute] confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Computing confusion matrix.\")\n",
    "\n",
    "max_encoded_val = max(cv_encoding.values())\n",
    "\n",
    "logger.debug(f\"{max_encoded_val=}\")\n",
    "\n",
    "# cm = confusion matrix\n",
    "cm_encoded_counts = np.bincount(conf_vol.ravel(), minlength=max_encoded_val + 1)\n",
    "\n",
    "cm_counts = {}\n",
    "\n",
    "for gt_pred_indices, enc_val in cv_encoding.items():\n",
    "    \n",
    "    cm_counts[gt_pred_indices] = cm_encoded_counts[enc_val]\n",
    "\n",
    "conf_matrix = [\n",
    "    [\n",
    "        cm_counts[(gt_idx, pred_idx)]\n",
    "        for pred_idx in labels_idx\n",
    "    ]\n",
    "    for gt_idx in labels_idx\n",
    "]\n",
    "\n",
    "conf_matrix = np.array(conf_matrix)\n",
    "\n",
    "try:\n",
    "    \n",
    "    assert (ncorrect_error_volume := (~error_volume).sum()) == (ncorrect_conf_matrix := conf_matrix.diagonal().sum()), (\n",
    "        f\"{ncorrect_error_volume=} {ncorrect_conf_matrix=}\"\n",
    "    )\n",
    "    \n",
    "except NameError as ex:\n",
    "    \n",
    "    if ex.args[0] != \"name 'error_volume' is not defined\":\n",
    "        raise ex\n",
    "    \n",
    "    # never mind...\n",
    "\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [save] confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving confusion matrix.\")\n",
    "estimation_volume[\"confusion_matrix_dtype\"] = str(conf_matrix.dtype)\n",
    "np.save(outputs.confusion_matrix, conf_matrix)\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [compute][save] roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing and saving ROC curves\")\n",
    "\n",
    "if opts.compute.roc_curve:\n",
    "    \n",
    "    roc_dfs = []\n",
    "\n",
    "    for label_idx in pbar(labels_idx):\n",
    "\n",
    "        logger.debug(f\"computing roc curve {label_idx=}\")\n",
    "\n",
    "        fpr, tpr, th = metrics.roc_curve(\n",
    "            labels_volume.ravel(), \n",
    "            probas_volume[:, :, :, label_idx].ravel(), \n",
    "\n",
    "            pos_label=label_idx,\n",
    "            drop_intermediate=True\n",
    "        )\n",
    "\n",
    "        roc_df = pd.DataFrame(\n",
    "            data={\n",
    "                \"fpr\": fpr,\n",
    "                \"tpr\": tpr,\n",
    "                \"th\": th,\n",
    "            }\n",
    "        ).T\n",
    "\n",
    "        logger.debug(f\"{label_idx=} {roc_df.shape=}\")\n",
    "        \n",
    "        roc_dfs.append(roc_df)\n",
    "        \n",
    "        roc_path = outputs.roc_curve(label_idx)\n",
    "\n",
    "        logger.debug(f\"saving roc curve {label_idx=} at {roc_path=}\")\n",
    "\n",
    "        roc_df.to_csv(\n",
    "            roc_path,\n",
    "            header=True,\n",
    "            index=True,\n",
    "        )\n",
    "\n",
    "    logger.info(\"done\")\n",
    "\n",
    "else: \n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] multi-class roc-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing the multiclass ROC curves\")\n",
    "\n",
    "if opts.compute.multiclass_roc_auc:\n",
    "\n",
    "    raveled_probas = probas_volume.reshape(-1, n_classes)\n",
    "    raveled_probas = raveled_probas / raveled_probas.sum(axis=-1, keepdims=True)  # more numerically precise...\n",
    "\n",
    "    multiclass_roc_auc_macro_ovr = t2s_analyse.multiclass_roc_auc_score(\n",
    "        y_true=labels_volume.ravel(),\n",
    "        y_score=raveled_probas,\n",
    "        average=\"macro\",\n",
    "        multi_class=\"ovr\",\n",
    "        labels=labels_idx,\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"{multiclass_roc_auc_macro_ovr=}\")\n",
    "\n",
    "    multiclass_roc_auc_macro_ovo = t2s_analyse.multiclass_roc_auc_score(\n",
    "        y_true=labels_volume.ravel(),\n",
    "        y_score=raveled_probas,\n",
    "        average=\"macro\",\n",
    "        multi_class=\"ovo\",\n",
    "        labels=labels_idx,\n",
    "    )\n",
    "\n",
    "    logger.debug(f\"{multiclass_roc_auc_macro_ovo=}\")\n",
    "    \n",
    "    logger.info(\"done\")\n",
    "\n",
    "else: \n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d error blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [compute] 2d error blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing 2d error blobs in the 3 directions.\")\n",
    "\n",
    "if opts.compute.error_blobs_2d_props:\n",
    "    error_2dblobs_props = analyse.get_2d_blob_props(\n",
    "        label_volume=error_volume,\n",
    "        data_volume=data_volume,\n",
    "        parallel_nprocs=parallel_nprocs,\n",
    "    )\n",
    "    logger.info(\"done\")\n",
    "\n",
    "else: \n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [save] 2d error blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Saving 2d error blobs.\")\n",
    "\n",
    "if opts.save.error_blobs_2d_props:\n",
    "    error_2dblobs_props.to_csv(outputs.error_2dblobs_props, index=False)\n",
    "    logger.info(\"done\")\n",
    "\n",
    "else: \n",
    "    logger.info(\"skipped\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d error blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [compute] 3d error blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [save] 3d error blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derived computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocs = None if not opts.compute.roc_curve else tuple(\n",
    "    {\n",
    "        \"tpr\": roc_df.loc[\"tpr\"].values,\n",
    "        \"fpr\": roc_df.loc[\"fpr\"].values,\n",
    "    }\n",
    "    for roc_df in roc_dfs\n",
    ")\n",
    "\n",
    "report_dict = t2s_analyse.get_classification_report(\n",
    "    cm=conf_matrix,\n",
    "    rocs=rocs,\n",
    ")\n",
    "\n",
    "if opts.compute.multiclass_roc_auc:\n",
    "    report_dict[\"macro\"][\"multiclass-roc-auc-ovr\"] = float(multiclass_roc_auc_macro_ovr)\n",
    "    report_dict[\"macro\"][\"multiclass-roc-auc-ovo\"] = float(multiclass_roc_auc_macro_ovo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, name in labels_idx_name.items():\n",
    "    report_dict[name] = report_dict[idx]\n",
    "    del report_dict[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Saving classification report.\")\n",
    "\n",
    "yaml_dump = functools.partial(\n",
    "    yaml.dump,\n",
    "    default_flow_style=False, \n",
    "    indent=4, \n",
    "    sort_keys=False\n",
    ")\n",
    "\n",
    "with outputs.classification_report_exact.open('w') as f:\n",
    "    yaml_dump(report_dict, f)\n",
    "\n",
    "with outputs.classification_report_human.open('w') as f:\n",
    "    humanized_report_str = yaml_dump(\n",
    "        report_dict, \n",
    "        Dumper=t2s_analyse.ClassifReportHumandDumper,\n",
    "    )\n",
    "    f.write(humanized_report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification report (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df,\n",
    "    table_human_simple,\n",
    "    table_human_detail\n",
    ") = analyse_pred.report2table(report_dict, labels_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    outputs.classification_report_table_csv, \n",
    "    header=True,\n",
    "    index=True,\n",
    ")\n",
    "\n",
    "table_str = tabulate.tabulate(table_human_simple, headers=cols)\n",
    "\n",
    "with outputs.classification_report_table_human.open(\"w\") as f:\n",
    "    f.write(table_str)\n",
    "\n",
    "table_str = tabulate.tabulate(table_human_detail, headers=cols)\n",
    "\n",
    "with outputs.classification_report_table_exact.open(\"w\") as f:\n",
    "    f.write(table_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_volume_alias = estimation_volume.fullname  # todo move me to args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    n_rows := 2, \n",
    "    n_cols := 2, \n",
    "    figsize=(n_cols * (sz := 4), n_rows * sz), \n",
    "    dpi=(dpi := 100),\n",
    "    gridspec_kw=dict(wspace=sz/30),\n",
    ")\n",
    "\n",
    "cm_display = t2s_viz.ConfusionMatrixDisplay(\n",
    "    cm_normalized := conf_matrix, \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format=None, \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[0, 0],\n",
    "    cmap_vmax=int(conf_matrix.max()),\n",
    ")\n",
    "\n",
    "cm_display.ax_.set_title(\"counts\")\n",
    "\n",
    "cm_display = t2s_viz.ConfusionMatrixDisplay(\n",
    "    cm_normalized := conf_matrix / conf_matrix.sum(), \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format='.2%', \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[0, 1]\n",
    ")\n",
    "\n",
    "cm_display.ax_.set_title(\"normalized (global)\")\n",
    "\n",
    "cm_display = t2s_viz.ConfusionMatrixDisplay(\n",
    "    cm_true_label_normalized := conf_matrix / conf_matrix.sum(axis=1).reshape(-1, 1), \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format='.1%', \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[1, 0],\n",
    ")\n",
    "cm_display.ax_.set_title(\"norm. by *GT* (line)\\ndiagonal = recall\")\n",
    "\n",
    "cm_display = t2s_viz.ConfusionMatrixDisplay(\n",
    "    cm_predicted_label_normalized := conf_matrix / conf_matrix.sum(axis=0).reshape(1, -1), \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format='.1%', \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[1, 1],\n",
    ")\n",
    "\n",
    "cm_display.ax_.set_title(\"norm. by *PRED* (column)\\ndiagonal = precision\")\n",
    "\n",
    "fig.suptitle(f\"Confusion matrices {estimation_volume_alias}\");\n",
    "\n",
    "fig.savefig(\n",
    "    fname=outputs.confusion_matrices_plot,\n",
    "    format=\"png\",\n",
    "    dpi=dpi,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"plotting roc curves\")\n",
    "\n",
    "if opts.compute.roc_curve:\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        n_rows := 1, \n",
    "        n_cols := 2, \n",
    "        figsize=(n_cols * (sz := 7), n_rows * sz), \n",
    "        dpi=(dpi := 130),\n",
    "    )\n",
    "\n",
    "    zoom = np.array(((0, .15), (.85, 1)))\n",
    "\n",
    "    fig.suptitle(\"Per class ROC curves\")\n",
    "\n",
    "    ax_full, ax_zoom = axs[0], axs[1]\n",
    "    ax_full.set_title(\"Full curve range [0, 1] x [0, 1]\")\n",
    "    ax_full.set_xlim(0, 1)\n",
    "    ax_full.set_ylim(0, 1)\n",
    "\n",
    "    ax_zoom.set_title(f\"Zoom on [{zoom[0, 0]}, {zoom[0, 1]}] x [{zoom[1, 0]}, {zoom[1, 1]}]\")\n",
    "    ax_zoom.set_xlim(*zoom[0])\n",
    "    ax_zoom.set_ylim(*zoom[1])\n",
    "\n",
    "    for label_idx, roc_df in zip(labels_idx, roc_dfs):\n",
    "\n",
    "        fpr = roc_df.loc[\"fpr\"].values\n",
    "        tpr = roc_df.loc[\"tpr\"].values\n",
    "\n",
    "        roc_display = metrics.RocCurveDisplay(\n",
    "            fpr=fpr, \n",
    "            tpr=tpr, \n",
    "            estimator_name=f\"{label_idx}\",\n",
    "        )\n",
    "\n",
    "        for ax in axs:\n",
    "            roc_display.plot(ax=ax)  \n",
    "\n",
    "    max_label_name_length = max(*map(len, labels_names))\n",
    "\n",
    "    for label_idx, roc_df in zip(labels_idx, roc_dfs):\n",
    "\n",
    "        fpr = roc_df.loc[\"fpr\"].values\n",
    "        tpr = roc_df.loc[\"tpr\"].values\n",
    "\n",
    "        label_name = labels_names[label_idx]\n",
    "\n",
    "        ax_full.get_legend().texts[label_idx].set_text(\n",
    "            label_name.ljust(max_label_name_length) +\n",
    "            f\"AUC={report_dict[label_name]['roc-auc']:.2%}\"\n",
    "        )\n",
    "    \n",
    "    ax_zoom.legend_ = None\n",
    "    \n",
    "    fig.savefig(outputs.roc_plot, format='png')\n",
    "\n",
    "    logger.info(\"done\")\n",
    "\n",
    "else:\n",
    "    logger.info(\"skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## volumetric fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_proportion_gt = conf_matrix.sum(axis=1)\n",
    "class_proportion_pred = conf_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_alias = estimation_volume.partition.alias  # todo move me to args\n",
    "volume_name = volume.fullname  # todo move me to args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    nrows := 1, ncols := 2, \n",
    "    figsize=(ncols * (sz := 7), nrows * sz), \n",
    "    dpi=(dpi := 90), \n",
    "    gridspec_kw=dict(wspace=sz/16, hspace=sz/12)\n",
    ")\n",
    "\n",
    "common_kwargs = dict(\n",
    "    barh_kwargs=dict(\n",
    "        height=.6,\n",
    "    ),\n",
    "    count_fmt_func=lambda c: f\"{humanize.intword(c)}\",\n",
    "    perc_fmt_func=lambda p: f\"{p:.1%}\",\n",
    ")\n",
    "\n",
    "display_gt = t2s_viz.ClassImbalanceDisplay(\n",
    "    volume_name=f\"ground truth\",\n",
    "    labels_idx=labels_idx,\n",
    "    labels_names=labels_names,\n",
    "    labels_counts=class_proportion_gt.tolist(),\n",
    ").plot(ax=axs[0], **common_kwargs)\n",
    "\n",
    "display_pred = t2s_viz.ClassImbalanceDisplay(\n",
    "    volume_name=f\"prediction\",\n",
    "    labels_idx=labels_idx,\n",
    "    labels_names=labels_names,\n",
    "    labels_counts=class_proportion_pred.tolist(),\n",
    ").plot(ax=axs[1], **common_kwargs)\n",
    "\n",
    "fig.suptitle(f\"{volume_name} ({partition_alias} set): volumetric fraction comparison.\")\n",
    "\n",
    "fig.savefig(\n",
    "    fname=outputs.volumetric_fraction_plot,\n",
    "    format=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - voxel size\n",
    "# - volume size \n",
    "# - fiber length\n",
    "# - fiber diameter\n",
    "# - porosity diameter\n",
    "# - fraction volumique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## adjacent layers correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [compute] adjacent layers correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AdjacentLayerCorrelation:\n",
    "    \n",
    "    class Dataset(Enum):\n",
    "        gt = 0\n",
    "        pred = 1\n",
    "    \n",
    "    dataset: str  # 'gt'/'pred'\n",
    "    axis: int\n",
    "    label: Optional[int]  \n",
    "        \n",
    "    values: List[int] = field(repr=False)\n",
    "        \n",
    "correlations = [\n",
    "    AdjacentLayerCorrelation(\n",
    "        dataset=AdjacentLayerCorrelation.Dataset.gt,\n",
    "        axis = axis,\n",
    "        label = label,\n",
    "        values = analyse_gt.adjacent_layers_correlation(\n",
    "            labels=labels_volume,\n",
    "            axis=axis,\n",
    "            nslices=1,\n",
    "            correlation_func=partial(analyse_gt.jaccard, label=label)\n",
    "        )\n",
    "    )\n",
    "    for axis, label in pbar(list(itertools.product(\n",
    "        list(range(3)),\n",
    "        [None] + list(labels_idx),\n",
    "    )))\n",
    "] + [\n",
    "    AdjacentLayerCorrelation(\n",
    "        dataset=AdjacentLayerCorrelation.Dataset.pred,\n",
    "        axis = axis,\n",
    "        label = label,\n",
    "        values = analyse_gt.adjacent_layers_correlation(\n",
    "            labels=labels_volume,\n",
    "            axis=axis,\n",
    "            nslices=1,\n",
    "            correlation_func=partial(analyse_gt.jaccard, label=label)\n",
    "        )\n",
    "    )\n",
    "    for axis, label in pbar(list(itertools.product(\n",
    "        list(range(3)),\n",
    "        [None] + list(labels_idx),\n",
    "    )))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [save] adjacent layers correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving adjacent layers correlation series.\")\n",
    "\n",
    "for corr in pbar(correlations):\n",
    "    \n",
    "    if corr.dataset != AdjacentLayerCorrelation.Dataset.pred:\n",
    "        continue\n",
    "    \n",
    "    filepath = outputs.layers_correlation(\n",
    "        axis=corr.axis,\n",
    "        label=corr.label,\n",
    "    )\n",
    "    \n",
    "    np.save(filepath, corr.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [plot] adjacent layers correlation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# todo make this a display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    nrows := n_classes + 1,\n",
    "    ncols := 3,\n",
    "    figsize = (\n",
    "        ncols * (sz := 5),\n",
    "        nrows * sz,\n",
    "    ),\n",
    "    dpi = 100,\n",
    ")\n",
    "\n",
    "\n",
    "def corr2ax(corr: AdjacentLayerCorrelation):\n",
    "    return axs[\n",
    "        corr.label if corr.label is not None else -1, \n",
    "        corr.axis\n",
    "    ]\n",
    "\n",
    "\n",
    "for corr in correlations:\n",
    "    ax = corr2ax(corr)\n",
    "    ax.plot(\n",
    "        corr.values,\n",
    "        label=f\"{corr.dataset.name}\",\n",
    "        linewidth=1,\n",
    "        linestyle='-' if corr.dataset == AdjacentLayerCorrelation.Dataset.gt else \":\",\n",
    "#         linestyle='-',\n",
    "#         linestyle=':',\n",
    "    )\n",
    "    \n",
    "for ax in axs.ravel():\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    \n",
    "for axis in range(3):\n",
    "    for label in list(range(n_classes)) + [None]:\n",
    "        ax_ = axs[label if label is not None else -1, axis]\n",
    "        ax_.set_title(\n",
    "            f\"{axis=} label={label if label is not None else 'all'}\"\n",
    "        )\n",
    "        \n",
    "fig.suptitle(f\"{volume_name} adjacent layer correlation\")\n",
    "        \n",
    "fig.savefig(fname=outputs.layers_correlation_plot, format=\"png\")\n",
    "plt.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [todo] measure the classification metrics layerwise in all the axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notable slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Finding notable slices.\")\n",
    "\n",
    "if opts.compute.error_blobs_2d_props:\n",
    "    \n",
    "    MIN_ERROR_BLOB_AREA = 1\n",
    "\n",
    "    logger.info(f'filtering error blobs < {MIN_ERROR_BLOB_AREA=}')\n",
    "\n",
    "    logger.debug(f\"before {(nblobs := error_2dblobs_props.shape[0])=} ({humanize.intcomma(nblobs)})\")\n",
    "\n",
    "    error_2dblobs_props = error_2dblobs_props[error_2dblobs_props.area > MIN_ERROR_BLOB_AREA]\n",
    "\n",
    "    logger.debug(f\"after {(nblobs := error_2dblobs_props.shape[0])=} ({humanize.intcomma(nblobs)})\")\n",
    "\n",
    "    if error_2dblobs_props.index.name != \"normal_axis\":\n",
    "        \n",
    "        error_2dblobs_props = error_2dblobs_props.reset_index().set_index([\"normal_axis\"])\n",
    "\n",
    "    notable_slices = {}\n",
    "\n",
    "    add_notable_slices = functools.partial(\n",
    "        t2s_analyse.add_notable_slices,\n",
    "        notable_slices=notable_slices,\n",
    "        error_2dblobs_props=error_2dblobs_props,\n",
    "    )\n",
    "\n",
    "    add_notable_slices_blobwise = functools.partial(\n",
    "        t2s_analyse.add_notable_slices_blobwise,\n",
    "        notable_slices=notable_slices,\n",
    "        error_2dblobs_props=error_2dblobs_props,\n",
    "    )\n",
    "\n",
    "    add_notable_slices_blobwise(t2s_analyse.max_area)\n",
    "\n",
    "    add_notable_slices_blobwise(partial(t2s_analyse.max_bbox_shape, dim=1), axes=(0,))\n",
    "    add_notable_slices_blobwise(partial(t2s_analyse.max_bbox_shape, dim=2), axes=(0,))\n",
    "    add_notable_slices_blobwise(partial(t2s_analyse.max_bbox_shape, dim=0), axes=(1,))\n",
    "    add_notable_slices_blobwise(partial(t2s_analyse.max_bbox_shape, dim=2), axes=(1,))\n",
    "    add_notable_slices_blobwise(partial(t2s_analyse.max_bbox_shape, dim=0), axes=(2,))\n",
    "    add_notable_slices_blobwise(partial(t2s_analyse.max_bbox_shape, dim=1), axes=(2,))\n",
    "\n",
    "    add_notable_slices_blobwise(t2s_analyse.max_major_axis_length)\n",
    "\n",
    "    add_notable_slices_blobwise(t2s_analyse.max_minor_axis_length)\n",
    "\n",
    "    add_notable_slices(t2s_analyse.max_error_area)\n",
    "\n",
    "    add_notable_slices(t2s_analyse.max_error_blob_avg_area)\n",
    "\n",
    "    with outputs.notable_slices_yaml.open('w') as f:\n",
    "        yaml_dump(notable_slices, f)\n",
    "    \n",
    "    logger.info(\"done\")\n",
    "    \n",
    "else: \n",
    "    logger.info(\"skipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot notable slices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"plotting notable slices\")\n",
    "\n",
    "for name, obj in notable_slices.items():\n",
    "    \n",
    "    logger.debug(f\"plotting {name}\")\n",
    "    \n",
    "    slice_ = 3 * [slice(None, None, None)]\n",
    "    slice_[obj[\"normal_axis\"]] = slice(obj[\"slice_idx\"], obj[\"slice_idx\"] + 1, None)\n",
    "    slice_ = tuple(slice_)\n",
    "\n",
    "    slice_data = data_volume[slice_].squeeze(obj[\"normal_axis\"])\n",
    "    slice_pred = preds_volume[slice_].squeeze(obj[\"normal_axis\"])\n",
    "    slice_err = error_volume[slice_].squeeze(obj[\"normal_axis\"])\n",
    "    \n",
    "    display_pred = t2s_viz.SliceDataPredictionDisplay(\n",
    "        slice_data=slice_data,\n",
    "        slice_prediction=slice_pred,\n",
    "        slice_name=name,  # todo: make each slice have some semantic name (\"max area = 420\")\n",
    "        n_classes=n_classes,\n",
    "    )\n",
    "    \n",
    "    display_err = t2s_viz.SliceDataPredictionDisplay(\n",
    "        slice_data=slice_data,\n",
    "        slice_prediction=slice_err,\n",
    "        slice_name=name,  # todo: make each slice have some semantic name (\"max area = 420\")\n",
    "        n_classes=n_classes,\n",
    "    )\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(sz := 15, 2 * sz), dpi=120)\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    display_pred.plot(axs[0], data_imshow_kwargs=dict(vmin=0, vmax=255))\n",
    "    display_err.plot(axs[1], data_imshow_kwargs=dict(vmin=0, vmax=255))\n",
    "    \n",
    "    fig.savefig(\n",
    "        fname = outputs.notable_slices_plot(name),\n",
    "        format=\"png\",\n",
    "        metadata=display_pred.metadata,\n",
    "    )       \n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack.notify(f\"notebook `{script_name}` finished in `{hostname}`!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
