{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import dataclasses\n",
    "from dataclasses import asdict, dataclass\n",
    "import functools\n",
    "import gc\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import pprint as pprint_module\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import humanize\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from cnn_segm import keras_custom_loss\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "from progressbar import progressbar as pbar\n",
    "from pymicro.file import file_utils\n",
    "import socket\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model as KerasModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from tomo2seg.process import reduce_dimensions \n",
    "from tomo2seg.args import ProcessVolumeArgs as Args\n",
    "from tomo2seg import viz as t2s_viz\n",
    "from tomo2seg.data import EstimationVolume\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.logger import add_file_handler as logger_add_file_handler\n",
    "from tomo2seg.logger import dict2str\n",
    "from tomo2seg.logger import logger\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import utils as tomo2seg_utils\n",
    "from tomo2seg import slackme\n",
    "from tomo2seg import slack\n",
    "from tomo2seg import volume_sequence\n",
    "from tomo2seg import hosts as t2s_hosts\n",
    "from tomo2seg import datasets as t2s_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "volume_name_version = t2s_datasets.VOLUME_COMPOSITE_V1\n",
    "model_name = \"paper-unet-2d.f16-stripping-layernorm.fold000.1612-341-593\"\n",
    "\n",
    "train_partition_name = None\n",
    "val_partition_name = None\n",
    "\n",
    "# [derived-input]\n",
    "train_partition_name = train_partition_name or 'train'\n",
    "val_partition_name = val_partition_name or 'val'\n",
    "\n",
    "volume_name = volume_name_version[0]\n",
    "volume_version = volume_name_version[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tomo2seg` objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomo2seg_model = Tomo2SegModel.build_from_model_name(model_name)\n",
    "\n",
    "volume = Volume.with_check(\n",
    "    name=volume_name, \n",
    "    version=volume_version\n",
    ")\n",
    "\n",
    "train_partition = volume[train_partition_name]\n",
    "val_partition = volume[val_partition_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = volume.nclasses\n",
    "\n",
    "output_dir = tomo2seg_model.model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_ok(property_): \n",
    "    \"\"\"\n",
    "    Make sure that a directory returned from a property exists.\n",
    "    \"\"\"\n",
    "    \n",
    "    @functools.wraps(property_)\n",
    "    def wrapper(self) -> Path:\n",
    "        dir_: Path = property_(self)\n",
    "        dir_.mkdir(exist_ok=True)\n",
    "        return dir_\n",
    "    \n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OutputFiles:\n",
    "    \n",
    "    root_dir: Path\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \n",
    "        assert self.root_dir.is_dir()\n",
    "        \n",
    "    @property\n",
    "    @mkdir_ok\n",
    "    def snapshots_root(self) -> Path:\n",
    "        return self.root_dir / \"snapshots-during-training\"\n",
    "        \n",
    "    @property\n",
    "    @mkdir_ok\n",
    "    def snapshots_dir(self) -> Path:\n",
    "        return self.snapshots_root / \"snapshots\"\n",
    "    \n",
    "    @property\n",
    "    @mkdir_ok\n",
    "    def snapshots_single_crop_dir(self) -> Path:\n",
    "        return self.snapshots_root / \"snapshots_single_crop_dir\"\n",
    "    \n",
    "    @property\n",
    "    @mkdir_ok\n",
    "    def snapshots_single_crop_from_each_dataset_dir(self) -> Path:\n",
    "        return self.snapshots_root / \"snapshots_single_crop_from_each_dataset_dir\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = OutputFiles(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "tf_strategy = tf.distribute.OneDeviceStrategy(\"/cpu:0\")\n",
    "logger.debug(f\"{tf_strategy=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tomo2seg_model.autosaved2_all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_model(model_path):\n",
    "\n",
    "    model = tf.keras.models.load_model(str(model_path), compile=False)\n",
    "    in_ = model.layers[0]\n",
    "    in_shape = in_.input_shape[0]\n",
    "    input_n_channels = in_shape[-1]\n",
    "    # make it capable of getting any dimension in the input\n",
    "    # \"-2\" = 1 for the batch size, 1 for the nb.channels\n",
    "    anysize_target_shape = (len(in_shape) - 2) * [None] + [input_n_channels]     \n",
    "    anysize_input = layers.Input(\n",
    "        shape=anysize_target_shape,\n",
    "        name=\"input_any_image_size\"\n",
    "    )\n",
    "    model.layers[0] = anysize_input\n",
    "    # this doesn't really matter bc this script will not fit the model\n",
    "    optimizer = optimizers.Adam()\n",
    "    loss_func = keras_custom_loss.jaccard2_loss\n",
    "    model.compile(loss=loss_func, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: move me up\n",
    "models_path_list = [\n",
    "    model_path\n",
    "    for model_path in tomo2seg_model.autosaved2_all()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf_strategy.scope():\n",
    "    logger.info(f\"Loading models with {tf_strategy.__class__.__name__}.\")\n",
    "    \n",
    "    keras_models = [\n",
    "        get_keras_model(model_path)\n",
    "        for model_path in pbar(models_path_list)\n",
    "    ]\n",
    "    \n",
    "    logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: move me up\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelSnapshot:\n",
    "    \n",
    "    filename: str\n",
    "    keras: KerasModel\n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        return self.filename < other.filename\n",
    "    \n",
    "    @property\n",
    "    def epoch_valloss(self) -> Tuple[int, float]:\n",
    "        epoch, loss = self.filename.split(\".\")[-3:-1]\n",
    "        epoch = int(epoch.split(\"-\")[-0])\n",
    "        loss = float(\"0.\" + loss)\n",
    "        return epoch, loss\n",
    "    \n",
    "    @property\n",
    "    def epoch(self) -> int:\n",
    "        return self.epoch_valloss[0]\n",
    "        \n",
    "    @property\n",
    "    def val_loss(self) -> float:\n",
    "        return self.epoch_valloss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = sorted([\n",
    "    ModelSnapshot(\n",
    "        filename=path.name,\n",
    "        keras=k_model,\n",
    "    )\n",
    "    for k_model, path in zip(keras_models, models_path_list)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: move me up\n",
    "data_path = volume.data_path\n",
    "data_dtype = volume.metadata.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loading data from disk at file: {data_path.name}\")\n",
    "logger.debug(f\"{data_path=}\")\n",
    "\n",
    "normalization_factor = volume_sequence.NORMALIZE_FACTORS[data_dtype]  # todo move to utils\n",
    "logger.debug(f\"{normalization_factor=}\")\n",
    "\n",
    "data_volume = file_utils.HST_read(\n",
    "    str(volume.data_path),  # it doesn't accept paths...\n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=volume.metadata.dtype,\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=True,\n",
    ") / normalization_factor  # normalize\n",
    "\n",
    "logger.debug(f\"{data_volume.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Cutting data with {train_partition.alias=} and {val_partition.alias=}\")  # todo: disentanble me, move me to utils\n",
    "logger.debug(f\"{train_partition=}\")\n",
    "logger.debug(f\"{val_partition=}\")\n",
    "\n",
    "data_volume_train = train_partition.get_volume_partition(data_volume)\n",
    "data_volume_val = val_partition.get_volume_partition(data_volume)\n",
    "    \n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the crops are not being serialized correctly...\n",
    "# this cells fixes it\n",
    "import csv\n",
    "\n",
    "# todo: move me up\n",
    "train_metacrop_history_path = tomo2seg_model.train_metacrop_history_path\n",
    "val_metacrop_history_path = tomo2seg_model.val_metacrop_history_path\n",
    "\n",
    "\n",
    "# todo: move to utils\n",
    "def modify_filename(filepath: Path, prefix: str = \"\", suffix: str = \"\") -> Path:\n",
    "    path, filename = os.path.split(filepath)\n",
    "    filename, extension = os.path.splitext(filename)\n",
    "    filename = f\"{prefix}{filename}{suffix}{extension}\"\n",
    "    return Path(path) / filename\n",
    "\n",
    "\n",
    "def fix_history_csv(in_filepath, out_filepath):\n",
    "    \n",
    "    with in_filepath.open(\"r\") as infile, out_filepath.open(\"w\") as outfile:\n",
    "\n",
    "        reader = csv.reader(infile, delimiter=\";\")  # it is not the \";\"\n",
    "        writer = csv.writer(outfile, delimiter=\";\")  # now it'll be\n",
    "\n",
    "        for line in pbar(reader, prefix=in_filepath.name):\n",
    "\n",
    "            pieces = line[0].split(\",\")\n",
    "\n",
    "            if len(pieces) > 9:  # not the header\n",
    "                pieces = [\n",
    "                    pieces[0],\n",
    "                    \",\".join(pieces[1:4]),\n",
    "                    \",\".join(pieces[4:7]),\n",
    "                    \",\".join(pieces[7:10]),\n",
    "                ] + pieces[10:]\n",
    "\n",
    "            writer.writerow(pieces)\n",
    "\n",
    "            \n",
    "fix_history_csv(\n",
    "    train_metacrop_history_path,\n",
    "    modify_filename(train_metacrop_history_path, suffix=\".fixed\")\n",
    ")\n",
    "            \n",
    "fix_history_csv(\n",
    "    val_metacrop_history_path,\n",
    "    modify_filename(val_metacrop_history_path, suffix=\".fixed\")\n",
    ")\n",
    "    \n",
    "train_metacrop_history_path = modify_filename(train_metacrop_history_path, suffix=\".fixed\")\n",
    "val_metacrop_history_path = modify_filename(val_metacrop_history_path, suffix=\".fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metacrop_hist = pd.read_csv(train_metacrop_history_path, sep=\";\")\n",
    "val_metacrop_hist = pd.read_csv(val_metacrop_history_path, sep=\";\")\n",
    "\n",
    "# get only the first batch in each epoch\n",
    "train_metacrop_hist = train_metacrop_hist[train_metacrop_hist.batch_idx == 0]\n",
    "val_metacrop_hist = val_metacrop_hist[val_metacrop_hist.batch_idx == 0]\n",
    "\n",
    "# get only the first sample in each batch\n",
    "batch_size = 10\n",
    "train_metacrop_hist = train_metacrop_hist[np.arange(train_metacrop_hist.shape[0]) % batch_size == 0]\n",
    "val_metacrop_hist = val_metacrop_hist[np.arange(val_metacrop_hist.shape[0]) % batch_size == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def line2meta_crop(line: str):\n",
    "    \n",
    "line = train_metacrop_hist.iloc[0].to_dict()\n",
    "\n",
    "def csv_line2obj(line):\n",
    "    import ast\n",
    "    del line['batch_idx']\n",
    "    del line['gt_type']\n",
    "\n",
    "    line['gt'] = volume_sequence.GT2D[line['gt']]\n",
    "    line['et'] = ast.literal_eval(line['et'])\n",
    "\n",
    "    for kw in ['x', 'y', 'z']:\n",
    "        line[kw] = slice(*[\n",
    "            int(x) \n",
    "            if x != \"None\" else\n",
    "            None\n",
    "            for x in [\n",
    "                x.strip()\n",
    "                for x in line[kw].split(\"(\")[1].split(\")\")[0].split(\",\")\n",
    "            ]\n",
    "        ])\n",
    "\n",
    "    return volume_sequence.MetaCrop3D(**line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_meta_crops = [csv_line2obj(line) for _, line in train_metacrop_hist.iterrows()]\n",
    "val_meta_crops = [csv_line2obj(line) for _, line in val_metacrop_hist.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta2crop = functools.partial(\n",
    "    volume_sequence.meta2crop,\n",
    "    is_label=False,\n",
    "    interpolation='spline',\n",
    ")\n",
    "\n",
    "\n",
    "def process_metacrop(metacrop, datavol, keras_model):\n",
    "    \n",
    "    crop_data = meta2crop(\n",
    "        metacrop, \n",
    "        volume=datavol,\n",
    "    )\n",
    "    crop_data = np.expand_dims(\n",
    "        crop_data,\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    crop_segm = keras_model.predict(crop_data)\n",
    "    crop_segm = crop_segm.squeeze().argmax(axis=-1)  \n",
    "    \n",
    "    return crop_data, crop_segm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Snapshot:\n",
    "    model: ModelSnapshot\n",
    "    train_crop_data: np.ndarray\n",
    "    train_crop_segm: np.ndarray\n",
    "    val_crop_data: np.ndarray\n",
    "    val_crop_segm: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = []\n",
    "\n",
    "for model in pbar(models):\n",
    "    epoch = model.epoch\n",
    "    \n",
    "    train_meta_crop = train_meta_crops[epoch]\n",
    "    \n",
    "    train_crop_data, train_crop_segm = process_metacrop(\n",
    "        train_meta_crop,\n",
    "        data_volume_train,\n",
    "        model.keras,\n",
    "    )\n",
    "    \n",
    "    val_meta_crop = val_meta_crops[epoch]\n",
    "\n",
    "    val_crop_data, val_crop_segm = process_metacrop(\n",
    "        val_meta_crop,\n",
    "        data_volume_val,\n",
    "        model.keras,\n",
    "    )\n",
    "    \n",
    "    snapshots.append(Snapshot(\n",
    "        model,\n",
    "        train_crop_data,\n",
    "        train_crop_segm,\n",
    "        val_crop_data,\n",
    "        val_crop_segm,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_single_crop = []\n",
    "\n",
    "val_meta_crop = train_meta_crop = volume_sequence.MetaCrop3D(\n",
    "    x=slice(512, 768, None),\n",
    "    y=slice(512, 768, None),\n",
    "    z=slice(0, 1, None),\n",
    "    et=None,\n",
    "    gt=volume_sequence.GT2D.identity,\n",
    "    vs=0,\n",
    "    is_2halfd=False,\n",
    ")\n",
    "\n",
    "for model in pbar(models):\n",
    "    \n",
    "    train_crop_data, train_crop_segm = process_metacrop(\n",
    "        train_meta_crop,\n",
    "        data_volume_train,\n",
    "        model.keras,\n",
    "    )\n",
    "    \n",
    "    val_crop_data, val_crop_segm = process_metacrop(\n",
    "        val_meta_crop,\n",
    "        data_volume_val,\n",
    "        model.keras,\n",
    "    )\n",
    "    \n",
    "    snapshots_single_crop.append(Snapshot(\n",
    "        model,\n",
    "        train_crop_data,\n",
    "        train_crop_segm,\n",
    "        val_crop_data,\n",
    "        val_crop_segm,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_single_crop_from_each_dataset = []\n",
    "\n",
    "train_meta_crop = train_meta_crops[0]\n",
    "val_meta_crop = val_meta_crops[0]\n",
    "\n",
    "for model in pbar(models):\n",
    "    \n",
    "    train_crop_data, train_crop_segm = process_metacrop(\n",
    "        train_meta_crop,\n",
    "        data_volume_train,\n",
    "        model.keras,\n",
    "    )\n",
    "    \n",
    "    val_crop_data, val_crop_segm = process_metacrop(\n",
    "        val_meta_crop,\n",
    "        data_volume_val,\n",
    "        model.keras,\n",
    "    )\n",
    "    \n",
    "    snapshots_single_crop_from_each_dataset.append(Snapshot(\n",
    "        model,\n",
    "        train_crop_data,\n",
    "        train_crop_segm,\n",
    "        val_crop_data,\n",
    "        val_crop_segm,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: move me up\n",
    "hist_df = pd.read_csv(tomo2seg_model.history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: move me up\n",
    "model_name = tomo2seg_model.name\n",
    "model_alias = \"2D (f16)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from typing import *\n",
    "from dataclasses import *\n",
    "from tomo2seg.viz import Axes, check_matplotlib_support\n",
    "from numpy import ndarray\n",
    "\n",
    "@dataclass\n",
    "class TrainingDisplay(t2s_viz.Display):\n",
    "    \"\"\"Structured inspired in `sklearn.metrics.RocCurveDisplay`\"\"\"\n",
    "\n",
    "    class XAxisMode(Enum):\n",
    "        epoch = 0\n",
    "        batch = 1\n",
    "        crop = 2\n",
    "        voxel = 3\n",
    "        time = 4\n",
    "\n",
    "    history: Dict[str, List]\n",
    "        \n",
    "    x_axis_mode: Union[XAxisMode, Tuple[XAxisMode]] = (XAxisMode.epoch,)\n",
    "    metrics: Tuple[str] = (\"loss\", \"val_loss\")    \n",
    "\n",
    "    model_name: Optional[str] = None\n",
    "\n",
    "    # not arguments\n",
    "    xs_: dict = field(init=False)\n",
    "    ys_: dict = field(init=False)\n",
    "    ax_: Axes = field(init=False)\n",
    "\n",
    "    def safe_get_from_history(self, key, assertion_types):\n",
    "        \n",
    "        assertion_types = (assertion_types,) if not isinstance(assertion_types, tuple) else assertion_types\n",
    "\n",
    "        try:\n",
    "            should_be_list = self.history[key]\n",
    "\n",
    "        except KeyError as ex:\n",
    "            msg = f\"The history dict given to {self.__class__.__name__} does not have {key=}.\"\n",
    "            logger.error(msg)\n",
    "            raise ex\n",
    "        \n",
    "        assert isinstance(should_be_list, list), f\"{type(should_be_list)=}\"\n",
    "        assert any(\n",
    "            isinstance(should_be_list[0], at)\n",
    "            for at in assertion_types\n",
    "        ), f\"{type(should_be_list[0])=} not \\in {assertion_types=}\"\n",
    "        \n",
    "        return should_be_list\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \n",
    "        mode = self.x_axis_mode\n",
    "        \n",
    "        self.x_axis_mode = mode = (mode,) if not isinstance(mode, tuple) else mode\n",
    "\n",
    "        for mod in mode:\n",
    "            assert isinstance(mod, self.XAxisMode), f\"{type(mod)=} in {self.x_axis_mode=}\"\n",
    "        \n",
    "        xs = {}\n",
    "\n",
    "        for mod in self.x_axis_mode:\n",
    "            \n",
    "            if mod == self.XAxisMode.epoch:\n",
    "                \n",
    "                try:\n",
    "                    x = self.safe_get_from_history(\"epoch\", int)\n",
    "\n",
    "                except KeyError as ex:\n",
    "\n",
    "                    n_epochs = len(self.history[\"loss\"])\n",
    "\n",
    "                    logger.warning(\n",
    "                        f\"{self._missing_signal_error_msg(ex.args[0], False)}\\n\"\n",
    "                        f\"Using a default sequence (0, 1, ..., {n_epochs - 1=})\"\n",
    "                    )\n",
    "                    \n",
    "                    x = np.range(n_epochs)\n",
    "                    \n",
    "                x = np.array(x)\n",
    "\n",
    "            elif mod == self.XAxisMode.batch:\n",
    "                \n",
    "                epoch_size = np.array(self.safe_get_from_history(\"train.epoch_size\", int))\n",
    "                x = np.cumsum(epoch_size)\n",
    "\n",
    "            elif mod == self.XAxisMode.crop:\n",
    "                \n",
    "                epoch_size = np.array(self.safe_get_from_history(\"train.epoch_size\", int))\n",
    "                batch_size = np.array(self.safe_get_from_history(\"train.batch_size\", int))\n",
    "                x = np.cumsum(epoch_size * batch_size)\n",
    "\n",
    "            elif mod == self.XAxisMode.voxel:\n",
    "                \n",
    "                epoch_size = np.array(self.safe_get_from_history(\"train.epoch_size\", int))\n",
    "                batch_size = np.array(self.safe_get_from_history(\"train.batch_size\", int))\n",
    "                train_crop_shape = self.safe_get_from_history(\"train.train_crop_shape\", tuple)\n",
    "                \n",
    "                n_voxels = np.array([\n",
    "                    shape[0] * shape[1] * shape[2]\n",
    "                    for shape in train_crop_shape\n",
    "                ])\n",
    "                x = np.cumsum(epoch_size * batch_size * n_voxels)\n",
    "\n",
    "            elif mod == self.XAxisMode.time:\n",
    "                \n",
    "                seconds = np.array(self.safe_get_from_history(\"seconds\", int))\n",
    "                x = np.cumsum(seconds)\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError(f\"{self.x_axis_mode=}\")\n",
    "\n",
    "            assert len(x) > 1, \"You don't have enough epochs to plot. Go to the gym and call me later.\"\n",
    "\n",
    "            xs[mod.name] = x\n",
    "\n",
    "        self.xs_ = xs\n",
    "        \n",
    "        metrics = self.metrics\n",
    "        \n",
    "        self.metrics = metrics = (metrics,) if not isinstance(metrics, tuple) else metrics\n",
    "\n",
    "        for met in metrics:\n",
    "            assert isinstance(met, str), f\"{type(met)=} in {self.metrics=}\"\n",
    "        \n",
    "        ys = {}\n",
    "        \n",
    "        for met in self.metrics:\n",
    "            ys[met] = np.array(self.safe_get_from_history(met, (int, float)))\n",
    "        \n",
    "        self.ys_ = ys\n",
    "\n",
    "    @property\n",
    "    def title(self) -> str:\n",
    "        return (self.model_name or \"\") + f\".training-plot\"\n",
    "\n",
    "    def plot(\n",
    "        self,\n",
    "        ax: ndarray,\n",
    "        metric_kwargs: dict = None,\n",
    "        val_metric_kwargs: dict = None,\n",
    "        n_xticks: int = 11,\n",
    "    ) -> \"TrainingHistoryDisplay\":\n",
    "        \n",
    "        check_matplotlib_support(this_func_name := f\"{(this_class_name := self.__class__.__name__)}.plot\")\n",
    "\n",
    "        assert isinstance(ax, Axes), f\"{type(ax)=}\"\n",
    "        \n",
    "        # i don't know why this is done, I just copied\n",
    "        self.ax_ = ax\n",
    "        self.fig_ = ax.figure\n",
    "\n",
    "        for metric_name in self.metrics:\n",
    "\n",
    "            x = self.xs_[self.x_axis_mode[0].name]\n",
    "            y = self.ys_[metric_name]\n",
    "            \n",
    "            split = \"val\" if metric_name.startswith(\"val_\") else \"train\"\n",
    "            \n",
    "            effective_kwargs = {\n",
    "                **dict(label=split),\n",
    "                **(\n",
    "                    (metric_kwargs if split == 'train' else val_metric_kwargs) \n",
    "                    or dict()\n",
    "                )\n",
    "            }\n",
    "\n",
    "            # noinspection PyArgumentList\n",
    "            self.plots_[metric_name] = ax.plot(x, y, **effective_kwargs)\n",
    "\n",
    "        tick_locator = plt.LinearLocator(numticks=n_xticks)\n",
    "        \n",
    "        x_tickss = [\n",
    "            tick_locator.tick_values(\n",
    "                vmin=min(x := self.xs_[mod.name]), \n",
    "                vmax=max(x),\n",
    "            )\n",
    "            for mod in self.x_axis_mode\n",
    "        ]\n",
    "\n",
    "        ax.set_xticks(x_tickss[0])\n",
    "\n",
    "        # format the ticks\n",
    "        x_tickss = [\n",
    "            [\n",
    "                str(int(val)) if mod == self.XAxisMode.epoch else\n",
    "                str(int(val)) if mod == self.XAxisMode.batch else\n",
    "                str(int(val / 1000)) + \"k\" if mod == self.XAxisMode.crop else\n",
    "                humanize.intword(int(float(f\"{val:.2g}\"))) if mod == self.XAxisMode.voxel else\n",
    "                humanize.time.naturaldelta(val, minimum_unit=\"seconds\") if mod == self.XAxisMode.time else\n",
    "                \"err\"\n",
    "                for val in ticks\n",
    "            ]\n",
    "            for ticks, mod in zip(x_tickss, self.x_axis_mode)\n",
    "        ]\n",
    "\n",
    "        # transpose\n",
    "        x_tickss = list(zip(*x_tickss))\n",
    "        x_ticks = [\"\\n\".join(strs) for strs in x_tickss]\n",
    "\n",
    "        ax.set_xticklabels(x_ticks)\n",
    "\n",
    "        ax.set_title(f\"training history{f': {self.model_name}' or ''}\")\n",
    "\n",
    "        ax.set_ylabel({', '.join(self.metrics)})\n",
    "        ax.set_xlabel(\"/\".join([mod.name for mod in self.x_axis_mode]))\n",
    "\n",
    "        # losses tend to go down, so this should be a good position\n",
    "        # notice that using default loc=None is slower\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(snapshot, dir_):\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows := 3, ncols := 2,\n",
    "        figsize=(\n",
    "            ncols * (sz := 5),\n",
    "            nrows * sz,\n",
    "        ),\n",
    "        dpi=100,\n",
    "    )\n",
    "\n",
    "    hist_gs = axs[2, 0].get_gridspec()\n",
    "\n",
    "    for ax in axs[2, :]:\n",
    "        ax.remove()\n",
    "\n",
    "    hist_ax = fig.add_subplot(hist_gs[2, :])\n",
    "    # fig.tight_layout()\n",
    "\n",
    "    hist_display = TrainingDisplay(\n",
    "        history=hist_df.to_dict('list'),\n",
    "        model_name=model_alias,\n",
    "    ).plot(ax=hist_ax)\n",
    "\n",
    "    hist_ax.set_yscale(\"log\")\n",
    "\n",
    "    hist_ax.vlines(snapshot.model.epoch, ymin=0, ymax=1, linestyle='--', color='gray')\n",
    "\n",
    "    train_axs = axs[0, :]\n",
    "    train_display = t2s_viz.SliceDataPredictionDisplay(\n",
    "        slice_data=snapshot.train_crop_data.squeeze(),  # todo move the squeeze to the processing\n",
    "        slice_prediction=snapshot.train_crop_segm,\n",
    "        slice_name=\"train\",\n",
    "        n_classes=n_classes,\n",
    "    ).plot(train_axs)\n",
    "    axs[0,1].set_title(f\"train (epoch={snapshot.model.epoch})\")\n",
    "\n",
    "    val_axs = axs[1, :]\n",
    "    val_display = t2s_viz.SliceDataPredictionDisplay(\n",
    "        slice_data=snapshot.val_crop_data.squeeze(),  # todo move the squeeze to the processing\n",
    "        slice_prediction=snapshot.val_crop_segm,\n",
    "        slice_name=\"val\",\n",
    "        n_classes=n_classes,\n",
    "    ).plot(val_axs)\n",
    "    axs[1,1].set_title(f\"val (epoch={snapshot.model.epoch})\")\n",
    "    \n",
    "    fig.suptitle(f\"{model_alias} (epoch={snapshot.model.epoch})\")\n",
    "\n",
    "    filepath = dir_ / f\"{model_name}.epoch{snapshot.model.epoch:03d}.png\"\n",
    "\n",
    "    fig.savefig(filepath, format=\"png\")       \n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snap in snapshots:\n",
    "    plot(snap, output_files.snapshots_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snap in snapshots_single_crop:\n",
    "    plot(snap, output_files.snapshots_single_crop_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snap in snapshots_single_crop_from_each_dataset:\n",
    "    plot(snap, output_files.snapshots_single_crop_from_each_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(path, '..')))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_ in [\n",
    "    output_files.snapshots_dir,\n",
    "    output_files.snapshots_single_crop_dir,\n",
    "    output_files.snapshots_single_crop_from_each_dataset_dir,\n",
    "]:\n",
    "    zip_ = dir_.parent / (dir_.name + \".zip\")\n",
    "    zipf = zipfile.ZipFile(zip_, 'w', zipfile.ZIP_DEFLATED)\n",
    "    zipdir(dir_, zipf)\n",
    "    zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
