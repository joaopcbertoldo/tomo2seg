{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import functools\n",
    "import gc\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from enum import Enum\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import PrettyPrinter, pprint\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import humanize\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "import yaml\n",
    "from matplotlib import cm, patches, pyplot as plt\n",
    "from numpy import ndarray\n",
    "from numpy.random import RandomState\n",
    "from progressbar import progressbar as pbar\n",
    "from pymicro.file import file_utils\n",
    "from sklearn import metrics, metrics as met, model_selection, preprocessing\n",
    "from skimage import measure as skimage_measure\n",
    "import tabulate\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import (\n",
    "    callbacks as keras_callbacks,\n",
    "    layers,\n",
    "    losses,\n",
    "    metrics as keras_metrics,\n",
    "    optimizers,\n",
    "    utils,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from yaml import YAMLObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from tomo2seg import (\n",
    "    analyse as tomo2seg_analyse,\n",
    "    callbacks as tomo2seg_callbacks,\n",
    "    data as tomo2seg_data,\n",
    "    losses as tomo2seg_losses,\n",
    "    schedule as tomo2seg_schedule,\n",
    "    slack,\n",
    "    slackme,\n",
    "    utils as tomo2seg_utils,\n",
    "    viz as tomo2seg_viz,\n",
    "    volume_sequence,\n",
    ")\n",
    "from tomo2seg.data import EstimationVolume, Volume\n",
    "from tomo2seg.logger import add_file_handler, dict2str, logger\n",
    "from tomo2seg.model import Model as Tomo2SegModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    pass\n",
    "\n",
    "from tomo2seg.datasets import (\n",
    "    VOLUME_COMPOSITE_V1 as VOLUME_NAME_VERSION,\n",
    "    VOLUME_COMPOSITE_V1_LABELS_REFINED3 as LABELS_VERSION,\n",
    "#     VOLUME_FRACTURE00_SEGMENTED00 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_FRACTURE00_SEGMENTED00_LABELS_REFINED3 as LABELS_VERSION,\n",
    ")\n",
    "\n",
    "volume_name, volume_version = VOLUME_NAME_VERSION\n",
    "labels_version = LABELS_VERSION\n",
    "\n",
    "random_state_seed = 42\n",
    "runid = int(time.time())\n",
    "# runid = 1607944057\n",
    "\n",
    "parallel_nprocs = 30\n",
    "\n",
    "logger.info(f\"{volume_name=}\")\n",
    "logger.info(f\"{volume_version=}\")\n",
    "logger.info(f\"{labels_version=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(\"../data\"):\n",
    "    if f\"vol={volume_name}.{volume_version}\" in f:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "# [manual-input]\n",
    "estimation_volume_fullname = \"vol=PA66GF30.v1.set=test.model=unet2halfd-sep.crop112-f16.fold000.1607-789-290.runid=1608-023-819\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "random_state = np.random.RandomState(random_state_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = Volume.with_check(\n",
    "    name=volume_name, version=volume_version\n",
    ")\n",
    "\n",
    "logger.debug(f\"volume=\\n{dict2str(asdict(volume))}\")\n",
    "\n",
    "estimation_volume = EstimationVolume.from_fullname(\n",
    "    estimation_volume_fullname\n",
    ")\n",
    "\n",
    "logger.debug(f\"estimation_volume=\\n{dict2str(asdict(estimation_volume))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_name = f\"{estimation_volume.fullname}.estimation-analysis\"\n",
    "exec_dir = estimation_volume.dir \n",
    "figs_dir = exec_dir\n",
    "\n",
    "logger.info(f\"{exec_name=}\")\n",
    "logger.info(f\"{exec_dir=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if estimation_volume.partition is None:\n",
    "    raise NotImplementedError(f\"{estimation_volume.partition=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_name = \"unet2d.vanilla03-f16.fold000.1606-505-109\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tomo2seg_model = Tomo2SegModel.build_from_model_name(model_name)\n",
    "logger.debug(f\"{tomo2seg_model=}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# data_volume = voldata_train\n",
    "# partition = volume.val_partition\n",
    "partition = volume.test_partition\n",
    "runid = 1606072056\n",
    "\n",
    "volume_and_partition_name = f\"{volume.fullname}.partition={partition.alias}\"\n",
    "\n",
    "logger.debug(f\"{partition=}\")\n",
    "logger.debug(f\"{partition.shape=}\")\n",
    "logger.debug(f\"{runid=}\")\n",
    "logger.debug(f\"{volume_and_partition_name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading data from disk.\")\n",
    "\n",
    "data_volume = file_utils.HST_read(\n",
    "    str(volume.data_path),  # it doesn't accept paths...\n",
    "    \n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=volume.metadata.dtype,\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{data_volume.shape=}\")\n",
    "\n",
    "if estimation_volume.partition is not None:\n",
    "    \n",
    "    logger.info(f\"Cutting data {estimation_volume.partition}.\")\n",
    "    \n",
    "    data_volume = estimation_volume.partition.get_volume_partition(data_volume)\n",
    "    \n",
    "    logger.debug(f\"{data_volume.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading labels from disk.\")\n",
    "\n",
    "labels_volume = file_utils.HST_read(\n",
    "    str(volume.versioned_labels_path(labels_version)),  # it doesn't accept paths...\n",
    "    \n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=\"uint8\",\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{labels_volume.shape=}\")\n",
    "\n",
    "if estimation_volume.partition is not None:\n",
    "    \n",
    "    logger.info(f\"Cutting labels {estimation_volume.partition}.\")\n",
    "    \n",
    "    labels_volume = estimation_volume.partition.get_volume_partition(labels_volume)\n",
    "    \n",
    "    logger.debug(f\"{labels_volume.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading predictions from disk.\")\n",
    "\n",
    "predictions_volume = file_utils.HST_read(\n",
    "    str(estimation_volume.predictions_path),  # it doesn't accept paths...\n",
    "    \n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=\"uint8\",\n",
    "    dims=estimation_volume.partition.shape if estimation_volume.partition is not None else volume.metadata.dimensions,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{predictions_volume.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading probabilities from disk.\")\n",
    "\n",
    "if estimation_volume.probabilities_path.exists():\n",
    "    \n",
    "    proba_dtype = np.float16\n",
    "    \n",
    "    # float16 instead of 64 to save memory\n",
    "    probas_volume = np.load(estimation_volume.probabilities_path).astype(proba_dtype)\n",
    "    \n",
    "    logger.debug(f\"{probas_volume.shape=}\")\n",
    "    \n",
    "    probabilities_are_available = True\n",
    "    \n",
    "else:\n",
    "    logger.warning(\"Probabilities are not available.\")\n",
    "    probabilities_are_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_idx = volume.metadata.labels\n",
    "labels_names = [volume.metadata.labels_names[idx] for idx in labels_idx]\n",
    "\n",
    "labels_idx_name = list(zip(labels_idx, labels_names))\n",
    "\n",
    "n_classes = len(labels_idx)\n",
    "\n",
    "logger.debug(f\"{n_classes=}\")\n",
    "logger.debug(f\"{labels_idx=}\")\n",
    "logger.debug(f\"{labels_names=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] confusion volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_indices = volume.metadata.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_classes > 100:\n",
    "    raise NotImplementedError(f\"{n_classes=}\")\n",
    "\n",
    "logger.info(\"Computing confusion volume encoding.\")\n",
    "\n",
    "cv_dtype = np.int16\n",
    "\n",
    "# cv = confusion volume\n",
    "cv_encoding = {\n",
    "    # (gt, pred)\n",
    "    (gt_idx, pred_idx): 100 * gt_idx + pred_idx\n",
    "    for gt_idx, pred_idx in itertools.product(labels_indices, labels_indices)\n",
    "}\n",
    "\n",
    "logger.debug(f\"cv_encoding=\\n{dict2str(cv_encoding)}\")\n",
    "estimation_volume[\"cv_encoding\"] = cv_encoding\n",
    "\n",
    "cv_encoding_inv = dict(map(lambda x: tuple(reversed(x)), cv_encoding.items()))\n",
    "\n",
    "logger.debug(f\"cv_encoding_inv=\\n{dict2str(cv_encoding_inv)}\")\n",
    "estimation_volume[\"cv_encoding_inv\"] = cv_encoding_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing confusion volume.\")\n",
    "\n",
    "# 10000 is an impossible encoding\n",
    "conf_vol = np.full_like(labels_volume, 10000, dtype=cv_dtype)\n",
    "\n",
    "for (gt_idx, pred_idx), encoded_value in pbar(\n",
    "    cv_encoding.items(),\n",
    "    max_value=len(cv_encoding)\n",
    "):\n",
    "\n",
    "    conf_vol[\n",
    "        (labels_volume == gt_idx) & (predictions_volume == pred_idx)\n",
    "    ] = encoded_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(conf_vol != 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [save] confusion volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving confusion volume.\")\n",
    "logger.debug(f\"{estimation_volume.confusion_volume_path=}\")\n",
    "\n",
    "file_utils.HST_write(conf_vol, str(estimation_volume.confusion_volume_path))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] error volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "error_volume = np.full_like(labels_volume, False, dtype=bool)\n",
    "\n",
    "for label_idx in pbar(labels_idx):\n",
    "    \n",
    "    encoded_value = cv_encoding[(label_idx, label_idx)]\n",
    "    \n",
    "    logger.debug(f\"{label_idx=} {encoded_value=}\")\n",
    "    \n",
    "    error_volume |= (conf_vol == encoded_value)\n",
    "    \n",
    "error_volume = ~error_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [save] error volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving error volume.\")\n",
    "logger.debug(f\"{estimation_volume.error_volume_path=}\")\n",
    "\n",
    "file_utils.HST_write(error_volume, str(estimation_volume.error_volume_path))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoded_val = max(cv_encoding.values())\n",
    "\n",
    "logger.debug(f\"{max_encoded_val=}\")\n",
    "\n",
    "# cm = confusion matrix\n",
    "cm_encoded_counts = np.bincount(conf_vol.ravel(), minlength=max_encoded_val + 1)\n",
    "\n",
    "cm_counts = {}\n",
    "\n",
    "for gt_pred_indices, enc_val in cv_encoding.items():\n",
    "    \n",
    "    cm_counts[gt_pred_indices] = cm_encoded_counts[enc_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = [\n",
    "    [\n",
    "        cm_counts[(gt_idx, pred_idx)]\n",
    "        for pred_idx in labels_indices\n",
    "    ]\n",
    "    for gt_idx in labels_indices\n",
    "]\n",
    "\n",
    "conf_matrix = np.array(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [save] confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving confusion matrix.\")\n",
    "logger.debug(f\"{estimation_volume.confusion_matrix_path=}\")\n",
    "\n",
    "estimation_volume[\"confusion_matrix_dtype\"] = conf_matrix.dtype\n",
    "\n",
    "np.save(estimation_volume.confusion_matrix_path, conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (ncorrect_error_volume := (~error_volume).sum()) == (ncorrect_conf_matrix := conf_matrix.diagonal().sum()), (\n",
    "    f\"{ncorrect_error_volume=} {ncorrect_conf_matrix=}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute-save] roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing and saving ROC curves\")\n",
    "\n",
    "roc_dfs = []\n",
    "\n",
    "for label_idx in pbar(labels_idx):\n",
    "    \n",
    "    logger.debug(f\"Computing roc curve {label_idx=}\")\n",
    "    \n",
    "    fpr, tpr, th = metrics.roc_curve(\n",
    "        labels_volume.ravel(), \n",
    "        probas_volume[:, :, :, label_idx].ravel(), \n",
    "        \n",
    "        pos_label=label_idx,\n",
    "        drop_intermediate=True\n",
    "    )\n",
    "    \n",
    "    roc_df = pd.DataFrame(\n",
    "        data={\n",
    "            \"fpr\": fpr,\n",
    "            \"tpr\": tpr,\n",
    "            \"th\": th,\n",
    "        }\n",
    "    ).T\n",
    "    \n",
    "    logger.debug(f\"{label_idx=} {roc_df.shape=}\")\n",
    "    \n",
    "    roc_path = estimation_volume.get_roc_curve_csv_path(label_idx)\n",
    "\n",
    "    logger.debug(f\"Saving roc curve of {label_idx=} at {roc_path=}\")\n",
    "    \n",
    "    roc_df.to_csv(\n",
    "        roc_path,\n",
    "        header=True,\n",
    "        index=True,\n",
    "    )\n",
    "    \n",
    "    roc_dfs.append(roc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] multi-class roc-auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "raveled_probas = probas_volume.reshape(-1, n_classes)\n",
    "raveled_probas = raveled_probas / raveled_probas.sum(axis=-1, keepdims=True)  # more numerically precise..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "multiclass_roc_auc_macro_ovr = tomo2seg_analyse.multiclass_roc_auc_score(\n",
    "    y_true=labels_volume.ravel(),\n",
    "    y_score=raveled_probas,\n",
    "    average=\"macro\",\n",
    "    multi_class=\"ovr\",\n",
    "    labels=labels_idx,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{multiclass_roc_auc_macro_ovr=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "multiclass_roc_auc_macro_ovo = tomo2seg_analyse.multiclass_roc_auc_score(\n",
    "    y_true=labels_volume.ravel(),\n",
    "    y_score=raveled_probas,\n",
    "    average=\"macro\",\n",
    "    multi_class=\"ovo\",\n",
    "    labels=labels_idx,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{multiclass_roc_auc_macro_ovo=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] 2d error blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack.notify(\"ready to launch parallel computation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Computing 2d error blobs in the 3 directions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "xy_blob_props = tomo2seg_analyse.get_slice_props_parallel(error_volume, data_volume, 2, nprocs=parallel_nprocs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "xz_blob_props = tomo2seg_analyse.get_slice_props_parallel(error_volume, data_volume, 1, nprocs=parallel_nprocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "compute"
    ]
   },
   "outputs": [],
   "source": [
    "yz_blob_props = tomo2seg_analyse.get_slice_props_parallel(error_volume, data_volume, 0, nprocs=parallel_nprocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Converting 2d blob props dicts to data frames.\")\n",
    "\n",
    "all_blob_props = [\n",
    "    yz_blob_props,  # normal axis x (0)\n",
    "    xz_blob_props,  #         ... y (1)\n",
    "    xy_blob_props,  #         ... z (2)\n",
    "]\n",
    "\n",
    "for axis in range(3):\n",
    "    \n",
    "    logger.debug(f\"{axis=}\")\n",
    "    \n",
    "    blob_props = all_blob_props[axis]\n",
    "    \n",
    "    ref_shape = len(blob_props[\"area\"])\n",
    "    \n",
    "    for k in blob_props.keys():\n",
    "        assert (shap := len(blob_props[k])) == ref_shape, f\"{k=} {shap=} {ref_shape=}\"\n",
    "    \n",
    "    all_blob_props[axis] = pd.DataFrame(blob_props)\n",
    "    \n",
    "    logger.debug(f\"{all_blob_props[axis].shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_2dblobs_props = pd.concat(\n",
    "    all_blob_props, \n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [save] 2d error blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(\"Saving 2d error blobs.\")\n",
    "\n",
    "filepath = estimation_volume.error_2dblobs_props_path\n",
    "\n",
    "logger.debug(f\"{filepath=}\")\n",
    "\n",
    "error_2dblobs_props.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [compute] 3d error blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [save] 3d error blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# derived computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dict = tomo2seg_analyse.get_classification_report(\n",
    "    cm=conf_matrix,\n",
    "    rocs=tuple(\n",
    "        {\n",
    "            \"tpr\": roc_df.loc[\"tpr\"].values,\n",
    "            \"fpr\": roc_df.loc[\"fpr\"].values,\n",
    "        }\n",
    "        for roc_df in roc_dfs\n",
    "    )\n",
    ")\n",
    "\n",
    "report_dict[\"macro\"][\"multiclass-roc-auc-ovr\"] = multiclass_roc_auc_macro_ovr\n",
    "report_dict[\"macro\"][\"multiclass-roc-auc-ovo\"] = multiclass_roc_auc_macro_ovo\n",
    "\n",
    "for idx, name in labels_idx_name:\n",
    "    report_dict[name] = report_dict[idx]\n",
    "    del report_dict[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_dump = functools.partial(\n",
    "    yaml.dump,\n",
    "    default_flow_style=False, \n",
    "    indent=4, \n",
    "    sort_keys=False\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Saving exact classification report.\"\n",
    "    f\"{estimation_volume.voxelwise_classification_report_exact=}\"\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Saving humanized classification report.\"\n",
    "    f\"\\n{estimation_volume.voxelwise_classification_report_human=}\"\n",
    ")\n",
    "\n",
    "with estimation_volume.voxelwise_classification_report_exact.open('w') as f:\n",
    "    yaml_dump(report_dict, f)\n",
    "\n",
    "with estimation_volume.voxelwise_classification_report_human.open('w') as f:\n",
    "    humanized_report_str = yaml_dump(\n",
    "        report_dict, \n",
    "        Dumper=tomo2seg_analyse.ClassifReportHumandDumper,\n",
    "    )\n",
    "    f.write(humanized_report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"classification report=\\n{humanized_report_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification report (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "table_human_simple = []\n",
    "table_human_detail = []\n",
    "\n",
    "cols0 = [\"class/average\"]\n",
    "\n",
    "cols1 = [\n",
    "    \"accuracy\",\n",
    "    \"precision\",\n",
    "    \"recall\",\n",
    "    \"f1\",\n",
    "    \"roc-auc\",\n",
    "    \"jaccard\",\n",
    "]\n",
    "\n",
    "cols2 = [\n",
    "    \"tp\",\n",
    "    \"fp\",\n",
    "    \"fn\",\n",
    "    \"tn\",\n",
    "]\n",
    "\n",
    "cols3 = [\n",
    "    \"support\",\n",
    "    \"npred\",\n",
    "]\n",
    "\n",
    "cols = cols0 + cols1 + cols2 + cols3\n",
    "\n",
    "for key in labels_names + [\"macro\", \"micro\"]:\n",
    "    \n",
    "    dic = report_dict[key]\n",
    "        \n",
    "    line = [key] + [\n",
    "        dic.get(m, None)\n",
    "        for m in cols1 + cols2 + cols3\n",
    "    ]\n",
    "    \n",
    "    line_human_simple = [\n",
    "        v if isinstance(v, str) else\n",
    "        (\n",
    "            f\"{humanize.intword(v)}\" + (\n",
    "                f\" ({dic[col_rel]:.1%})\" if col in (\"tp\", \"fp\", \"fn\", \"tn\") and (col_rel := col + '_relative') in dic else \"\"\n",
    "            )\n",
    "        ) if isinstance(v, int) else \n",
    "        f\"{v:.1%}\" if v is not None else \n",
    "        \"-\"\n",
    "        for v, col in zip(line, cols)\n",
    "    ]\n",
    "    \n",
    "    line_human_detail = [\n",
    "        v if isinstance(v, str) else\n",
    "        (\n",
    "            f\"{humanize.intcomma(v)} ({humanize.intword(v)})\" + (\n",
    "                f\" ({dic[col_rel]:.4%})\" if col in (\"tp\", \"fp\", \"fn\", \"tn\") and (col_rel := col + '_relative') in dic else \"\"\n",
    "            )\n",
    "        ) if isinstance(v, int) else \n",
    "        f\"{v:.4%}\" if v is not None else \n",
    "        \"-\"\n",
    "        for v, col in zip(line, cols)\n",
    "    ]\n",
    "\n",
    "\n",
    "    table.append(line)\n",
    "    table_human_simple.append(line_human_simple)\n",
    "    table_human_detail.append(line_human_detail)\n",
    "    \n",
    "table_human_simple.insert(-2, [])\n",
    "table_human_detail.insert(-2, [])\n",
    "\n",
    "df = pd.DataFrame(table, columns=cols).set_index(cols0[0])\n",
    "\n",
    "logger.debug(f\"{estimation_volume.classification_report_table_exact_csv_path=}\")\n",
    "\n",
    "df.to_csv(\n",
    "    estimation_volume.classification_report_table_exact_csv_path, \n",
    "    header=True,\n",
    "    index=True,\n",
    ")\n",
    "\n",
    "table_str = tabulate.tabulate(table_human_simple, headers=cols)\n",
    "logger.info(table_str)\n",
    "\n",
    "logger.debug(f\"{estimation_volume.classification_report_table_human_simple_txt_path=}\")\n",
    "\n",
    "with estimation_volume.classification_report_table_human_simple_txt_path.open(\"w\") as f:\n",
    "    f.write(table_str)\n",
    "\n",
    "table_str = tabulate.tabulate(table_human_detail, headers=cols)\n",
    "logger.info(table_str)\n",
    "\n",
    "logger.debug(f\"{estimation_volume.classification_report_table_human_detail_txt_path=}\")\n",
    "\n",
    "with estimation_volume.classification_report_table_human_detail_txt_path.open(\"w\") as f:\n",
    "    f.write(table_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f\"Saving figure {(fig_name := 'confusion-matrix.png')=}\")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    n_rows := 2, \n",
    "    n_cols := 2, \n",
    "    figsize=(n_cols * (sz := 7), n_rows * sz), \n",
    "    dpi=(dpi := 120),\n",
    "    gridspec_kw=dict(wspace=sz/30),\n",
    ")\n",
    "\n",
    "cm_display = tomo2seg_viz.ConfusionMatrixDisplay(\n",
    "    cm_normalized := conf_matrix, \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format=None, \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[0, 0],\n",
    "    cmap_vmax=int(conf_matrix.max()),\n",
    ")\n",
    "\n",
    "cm_display.ax_.set_title(\"Confusion matrix\")\n",
    "\n",
    "cm_display = tomo2seg_viz.ConfusionMatrixDisplay(\n",
    "    cm_normalized := conf_matrix / conf_matrix.sum(), \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format='.2%', \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[0, 1]\n",
    ")\n",
    "\n",
    "cm_display.ax_.set_title(\"Confusion matrix (normalized)\")\n",
    "\n",
    "cm_display = tomo2seg_viz.ConfusionMatrixDisplay(\n",
    "    cm_true_label_normalized := conf_matrix / conf_matrix.sum(axis=1).reshape(-1, 1), \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format='.1%', \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[1, 0],\n",
    ")\n",
    "cm_display.ax_.set_title(\"Confusion matrix normalized (%)\\nby *true label* (by line)\\ndiagonal = recall\")\n",
    "\n",
    "cm_display = tomo2seg_viz.ConfusionMatrixDisplay(\n",
    "    cm_predicted_label_normalized := conf_matrix / conf_matrix.sum(axis=0).reshape(1, -1), \n",
    "    display_labels=labels_names,\n",
    ").plot(\n",
    "    values_format='.1%', \n",
    "    cmap=cm.inferno, \n",
    "    ax=axs[1, 1],\n",
    ")\n",
    "\n",
    "cm_display.ax_.set_title(\"Confusion matrix normalized (%)\\nby *predicted label* (by column)\\ndiagonal = precision\")\n",
    "\n",
    "fig.suptitle(f\"Confusion matrices {estimation_volume_fullname}\");\n",
    "\n",
    "fig.savefig(\n",
    "    fname=figs_dir / fig_name,\n",
    "    dpi=dpi,\n",
    "    metadata={\"Title\": f\"vol={volume.fullname}::analysis::{fig_name}\",}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "manual-input"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"Saving figure of ROC curves at {(fig_name := f'roc-curves.png')}\")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    n_rows := 1, \n",
    "    n_cols := 2, \n",
    "    figsize=(n_cols * (sz := 7), n_rows * sz), \n",
    "    dpi=(dpi := 130),\n",
    ")\n",
    "\n",
    "# [manual-input]\n",
    "zoom = np.array(((0, .15), (.85, 1)))\n",
    "\n",
    "fig.suptitle(\"Per class ROC curves\")\n",
    "\n",
    "ax_full, ax_zoom = axs[0], axs[1]\n",
    "ax_full.set_title(\"Full curve range [0, 1] x [0, 1]\")\n",
    "ax_full.set_xlim(0, 1)\n",
    "ax_full.set_ylim(0, 1)\n",
    "\n",
    "ax_zoom.set_title(f\"Zoom on [{zoom[0, 0]}, {zoom[0, 1]}] x [{zoom[1, 0]}, {zoom[1, 1]}]\")\n",
    "ax_zoom.set_xlim(*zoom[0])\n",
    "ax_zoom.set_ylim(*zoom[1])\n",
    "\n",
    "for label_idx, roc_df in zip(labels_idx, roc_dfs):\n",
    "    \n",
    "    fpr = roc_df.loc[\"fpr\"].values\n",
    "    tpr = roc_df.loc[\"tpr\"].values\n",
    "    \n",
    "    roc_display = metrics.RocCurveDisplay(\n",
    "        fpr=fpr, \n",
    "        tpr=tpr, \n",
    "        estimator_name=f\"{label_idx}\",\n",
    "    )\n",
    "    \n",
    "    for ax in axs:\n",
    "        roc_display.plot(ax=ax)  \n",
    "\n",
    "max_label_name_length = max(*map(len, labels_names))\n",
    "\n",
    "for label_idx, roc_df in zip(labels_idx, roc_dfs):\n",
    "    \n",
    "    fpr = roc_df.loc[\"fpr\"].values\n",
    "    tpr = roc_df.loc[\"tpr\"].values\n",
    "    \n",
    "    label_name = labels_names[label_idx]\n",
    "    \n",
    "    ax_full.get_legend().texts[label_idx].set_text(\n",
    "        label_name.ljust(max_label_name_length) +\n",
    "        f\"AUC={report_dict[label_name]['roc-auc']:.2%}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "ax_zoom.legend_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## volumetric fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_proportion_gt = conf_matrix.sum(axis=1)\n",
    "class_proportion_pred = conf_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(\n",
    "    nrows := 1, ncols := 2, \n",
    "    figsize=(ncols * (sz := 7), nrows * sz), \n",
    "    dpi=(dpi := 90), \n",
    "    gridspec_kw=dict(wspace=sz/16, hspace=sz/12)\n",
    ")\n",
    "\n",
    "display_gt = tomo2seg_viz.ClassImbalanceDisplay(\n",
    "    volume_name=f\"{volume.fullname} (partition={estimation_volume.partition.alias})\",\n",
    "    labels_idx=labels_idx,\n",
    "    labels_names=labels_names,\n",
    "    labels_counts=class_proportion_gt.tolist(),\n",
    ").plot(ax[0])\n",
    "\n",
    "\n",
    "model_name_idx = estimation_volume.fullname.index(\"model\")\n",
    "\n",
    "display_pred = tomo2seg_viz.ClassImbalanceDisplay(\n",
    "    volume_name=f\"{estimation_volume.fullname[:model_name_idx-1]}\\n{estimation_volume.fullname[model_name_idx:]}\",\n",
    "    labels_idx=labels_idx,\n",
    "    labels_names=labels_names,\n",
    "    labels_counts=class_proportion_pred.tolist(),\n",
    ").plot(ax[1])\n",
    "\n",
    "fig.suptitle(f\"Volumetric fraction comparison.\")\n",
    "\n",
    "logger.info(f\"Saving figure {(figname := 'volumetric-fraction.png')=}\")\n",
    "\n",
    "fig.savefig(\n",
    "    fname=figs_dir / figname,\n",
    "    format=\"png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physical metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- voxel size\n",
    "- volume size \n",
    "- fiber length\n",
    "- fiber diameter\n",
    "- porosity diameter\n",
    "- fraction volumique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notable slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Finding notable slices.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_2dblobs_props = pd.read_csv(estimation_volume.error_2dblobs_props_path)\n",
    "\n",
    "MIN_ERROR_BLOB_AREA = 1\n",
    "\n",
    "logger.info(f'filtering error blobs < {MIN_ERROR_BLOB_AREA=}')\n",
    "\n",
    "logger.debug(f\"before {(nblobs := error_2dblobs_props.shape[0])=} ({humanize.intcomma(nblobs)})\")\n",
    "\n",
    "error_2dblobs_props = error_2dblobs_props[error_2dblobs_props.area > MIN_ERROR_BLOB_AREA]\n",
    "\n",
    "logger.debug(f\"after {(nblobs := error_2dblobs_props.shape[0])=} ({humanize.intcomma(nblobs)})\")\n",
    "\n",
    "error_2dblobs_props = error_2dblobs_props.set_index([\"normal_axis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `add_notable_slices`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_slices = {}\n",
    "\n",
    "def add_notable_slices(func: Callable[[pd.DataFrame], Tuple[dict, dict]], axes=(0, 1, 2)):\n",
    "    for axis in axes:\n",
    "        \n",
    "        name, slice_idx, custom_attrs = func(error_2dblobs_props.loc[axis])\n",
    "\n",
    "        name += f\".{axis=}\"\n",
    "\n",
    "        notable_slice_dict = notable_slices[name] = {\n",
    "            \"name\": name,\n",
    "            \"normal_axis\": axis,\n",
    "            \"slice_idx\": int(slice_idx),\n",
    "        }\n",
    "        \n",
    "        slice_ = [slice(None), slice(None), slice(None)]\n",
    "        slice_[axis] = slice(slice_idx, slice_idx + 1)\n",
    "        slice_ = tuple(slice_)\n",
    "        \n",
    "        notable_slice_dict.update({\n",
    "            \"slice\": slice_,\n",
    "            **custom_attrs,\n",
    "        })\n",
    "        \n",
    "\n",
    "def add_notable_slices_blobwise(func: Callable[[pd.DataFrame], Tuple[dict, dict]], axes=(0, 1, 2)):\n",
    "    for axis in axes:\n",
    "        \n",
    "        name, row, custom_attrs = func(error_2dblobs_props.loc[axis])\n",
    "\n",
    "        name += f\".{axis=}\"\n",
    "\n",
    "        notable_slice_dict = notable_slices[name] = {\n",
    "            \"name\": name,\n",
    "            \"normal_axis\": axis,\n",
    "            \"slice_idx\": int(row.slice_idx),\n",
    "        }\n",
    "\n",
    "        centroid3d = tuple(\n",
    "            int(row[f\"centroid-{axx}\"])\n",
    "            for axx in range(3)\n",
    "        )\n",
    "\n",
    "        centroid2d = tuple(\n",
    "            val \n",
    "            for axx, val in enumerate(centroid3d)\n",
    "            if axx != axis\n",
    "        )\n",
    "\n",
    "        bbox3d = (\n",
    "            slice(int(row[f\"bbox-0\"]), int(row[f\"bbox-3\"])),\n",
    "            slice(int(row[f\"bbox-1\"]), int(row[f\"bbox-4\"])),\n",
    "            slice(int(row[f\"bbox-2\"]), int(row[f\"bbox-5\"])),\n",
    "        )\n",
    "\n",
    "        bbox2d = tuple(\n",
    "            val \n",
    "            for axx, val in enumerate(bbox3d)\n",
    "            if axx != axis\n",
    "        )\n",
    "\n",
    "        notable_slice_dict.update({\n",
    "            \"centroid3d\": centroid3d,\n",
    "            \"centroid2d\": centroid2d,\n",
    "            \"bbox3d\": bbox3d,\n",
    "            \"bbox2d\": bbox2d,\n",
    "            **custom_attrs,\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blobwise criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error blob max area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_area(df):\n",
    "    row = df.iloc[df.area.argmax()]\n",
    "    custom_attrs = {\"blob-area\": int(row[\"area\"])}\n",
    "    return \"error-blob.max-area\", row, custom_attrs\n",
    "\n",
    "add_notable_slices_blobwise(max_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error blob max bbox.shape[i] for i in  {0, 1, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_bbox_shape(df, dim):\n",
    "    \n",
    "    bbox_shape_dim = df[f\"bbox-{dim + 3}\"] - df[f\"bbox-{dim}\"] \n",
    "    \n",
    "    arg_max = bbox_shape_dim.argmax()\n",
    "    row = df.iloc[arg_max]\n",
    "    \n",
    "    custom_attrs = {f\"blob-bbox.length.axis={dim}\": int(bbox_shape_dim.iloc[arg_max])}\n",
    "    \n",
    "    return f\"error-blob.max-bbox-lenghth-axis={axis}\", row, custom_attrs\n",
    "\n",
    "add_notable_slices_blobwise(partial(max_bbox_shape, dim=1), axes=(0,))\n",
    "add_notable_slices_blobwise(partial(max_bbox_shape, dim=2), axes=(0,))\n",
    "add_notable_slices_blobwise(partial(max_bbox_shape, dim=0), axes=(1,))\n",
    "add_notable_slices_blobwise(partial(max_bbox_shape, dim=2), axes=(1,))\n",
    "add_notable_slices_blobwise(partial(max_bbox_shape, dim=0), axes=(2,))\n",
    "add_notable_slices_blobwise(partial(max_bbox_shape, dim=1), axes=(2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error blob max `major_axis_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_major_axis_length(df):\n",
    "    row = df.iloc[df.major_axis_length.argmax()]\n",
    "    custom_attrs = {\"blob-major-axis-length\": float(row[\"major_axis_length\"])}\n",
    "    return \"error-blob.max-major-axis-length\", row, custom_attrs\n",
    "\n",
    "add_notable_slices_blobwise(max_major_axis_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error blob max `minor_axis_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_minor_axis_length(df):\n",
    "    row = df.iloc[df.minor_axis_length.argmax()]\n",
    "    custom_attrs = {\"blob-minor-axis-length\": float(row[\"minor_axis_length\"])}\n",
    "    return \"error-blob.max-minor-axis-length\", row, custom_attrs\n",
    "\n",
    "add_notable_slices_blobwise(max_minor_axis_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max error area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_error_area(df):\n",
    "    area_per_slice = df[[\"area\", \"slice_idx\"]].groupby(\"slice_idx\").sum()\n",
    "    slice_idx = area_per_slice.area.argmax()\n",
    "    custom_attrs = {\"slice-error-area\": int(area_per_slice.loc[slice_idx].area)}\n",
    "    return \"max-error-area\", slice_idx, custom_attrs\n",
    "\n",
    "add_notable_slices(max_error_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## max error blob avg area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_error_blob_avg_area(df):\n",
    "    avg_blob_area_per_slice = df[[\"area\", \"slice_idx\"]].groupby(\"slice_idx\").mean()\n",
    "    slice_idx = avg_blob_area_per_slice.area.argmax()\n",
    "    custom_attrs = {\"slice-avg-error-blob-area\": int(avg_blob_area_per_slice.loc[slice_idx].area)}\n",
    "    return \"max-avg-error-blob-area\", slice_idx, custom_attrs\n",
    "\n",
    "add_notable_slices(max_error_blob_avg_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [save] notable slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "save"
    ]
   },
   "outputs": [],
   "source": [
    "logger.info(f\"{dict2str(notable_slices)}\")\n",
    "\n",
    "estimation_volume[\"notable-slices\"] = notable_slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_name = \"analyse-estimation-00-tomo88-a.ipynb\"\n",
    "\n",
    "import os\n",
    "this_dir = os.getcwd()\n",
    "logger.warning(f\"{this_nb_name=} {this_dir=}\")\n",
    "\n",
    "os.system(f\"jupyter nbconvert {this_dir}/{this_nb_name} --output-dir {str(exec_dir)} --to html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack.notify(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Per-class predicted probabilities histograms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.info(\"Computing probability histograms per label.\")\n",
    "probas_volume_reshaped = probas_volume.reshape(-1, n_classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "histograms = []\n",
    "bins = np.linspace(0, 1, 101)\n",
    "\n",
    "for label_idx in labels_idx:\n",
    "    logger.debug(f\"Computing histogram for {label_idx=}\")\n",
    "    \n",
    "    selector = labels_volume.ravel() == label_idx\n",
    "    \n",
    "    label_hists = []\n",
    "    for class_proba_idx in labels_idx:\n",
    "        values, _ = np.histogram(\n",
    "            probas_volume_reshaped[selector, class_proba_idx], bins=bins\n",
    "        ) \n",
    "        label_hists.append(values / values.sum())\n",
    "        \n",
    "    histograms.append(label_hists)\n",
    "    \n",
    "histograms = np.array(histograms)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.info(f\"Saving probabilities histograms at `{estimation_volume.probabilities_histograms_path=}`\")\n",
    "np.save(\n",
    "    estimation_volume.probabilities_histograms_path,\n",
    "    histograms\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig, axs = plt.subplots(\n",
    "    nrows := 3, \n",
    "    ncols := 1, \n",
    "    figsize=(2 * ncols * (sz := 5), nrows * sz), \n",
    "    dpi=200,\n",
    "    gridspec_kw=dict(wspace=0, hspace=sz/12)\n",
    ")\n",
    "\n",
    "for label_idx, label_name, label_n_voxels, ax in zip(labels_idx, labels_names, labels_counts, axs.ravel()):\n",
    "    \n",
    "    logger.debug(f\"Plotting histograms for {label_idx=}\")\n",
    "    \n",
    "    display = viz.ClassProbabilityHistogramDisplay(\n",
    "        bins=bins.tolist(),\n",
    "        values_per_class=histograms[label_idx].tolist(),\n",
    "        labels_idx=labels_idx,\n",
    "        labels_names=labels_names,\n",
    "    ).plot(ax)\n",
    "    \n",
    "    title_complement = f\" label={label_name}\"\n",
    "    title_complement += f\"\\n#voxels = {humanize.intword(label_n_voxels)} ({humanize.intcomma(label_n_voxels)})\"\n",
    "    \n",
    "    ax.set_title(ax.get_title() + title_complement)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Class probability histogram per label.\"\n",
    "    \"\\nEach graph only considers the pixels from a given (known) label (ground truth).\"\n",
    ")\n",
    "    \n",
    "logger.info(f\"Saving figure {(figname := display.title + '.png')=}\")\n",
    "display.fig_.savefig(\n",
    "    fname=figs_dir / figname,\n",
    "    format=\"png\",\n",
    "    metadata=display.metadata,\n",
    ")    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Prediction normalized entropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.info(\"Computing voxels' probability distribution normalized entropy.\")\n",
    "\n",
    "# probas_volume_t = tf.convert_to_tensor(\n",
    "#     value=probas_volume,\n",
    "#     dtype=tf.float32,\n",
    "# )\n",
    "\n",
    "eps = 1e-6\n",
    "max_entropy = - np.log(1 / (n_classes + eps))\n",
    "\n",
    "# tensors are faster\n",
    "normalized_voxel_entropies = - (probas_volume * np.log(probas_volume + eps)).sum(axis=-1) / max_entropy\n",
    "\n",
    "# entropy\n",
    "# normalized_voxel_entropies_t = - tf.math.reduce_sum(\n",
    "#     tf.math.multiply(probas_volume_t, tf.math.log(probas_volume_t + eps)), \n",
    "#     axis=-1\n",
    "# )\n",
    "\n",
    "# normalize it\n",
    "# normalized_voxel_entropies_t = tf.math.divide(normalized_voxel_entropies_t, max_entropy)\n",
    "\n",
    "# normalized_voxel_entropies = normalized_voxel_entropies_t.numpy()\n",
    "\n",
    "# del probas_volume_t\n",
    "\n",
    "logger.debug(f\"{normalized_voxel_entropies.shape=}\")\n",
    "\n",
    "path = str(estimation_volume.voxel_normalized_entropy_path)\n",
    "logger.info(f\"Saving normalized entropies volume to {path=}\")\n",
    "\n",
    "file_utils.HST_write(\n",
    "    # it has to be 32 to read it in fiji \n",
    "    normalized_voxel_entropies.astype(np.float32), \n",
    "    path\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.debug(\"Computing normalized entropy histograms per label.\")\n",
    "\n",
    "entropy_histograms = []\n",
    "\n",
    "normalized_voxel_entropies_t = tf.reshape(normalized_voxel_entropies_t, (-1,))\n",
    "\n",
    "range_ = tf.constant([0., 1.], dtype=normalized_voxel_entropies_t.dtype)\n",
    "nbins = 100\n",
    "\n",
    "range_np = range_.numpy()\n",
    "bins_borders = np.linspace(range_np[0], range_np[1], nbins + 1, endpoint=True)\n",
    "\n",
    "for label_idx in labels_idx:\n",
    "    logger.debug(f\"Computing histogram for {label_idx=}\")\n",
    "    \n",
    "    selector = (labels_volume == label_idx).ravel()\n",
    "    \n",
    "    entropy_hist_t = tf.histogram_fixed_width(\n",
    "        values=normalized_voxel_entropies_t[selector],\n",
    "        value_range=range_,\n",
    "        nbins=nbins,\n",
    "        dtype=tf.int32,\n",
    "    )\n",
    "    entropy_hist_t = tf.math.divide(entropy_hist_t, tf.math.reduce_sum(entropy_hist_t))\n",
    "    entropy_histograms.append(entropy_hist_t.numpy())\n",
    "    \n",
    "entropy_histograms = np.array(entropy_histograms)\n",
    "\n",
    "del normalized_voxel_entropies_t"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.info(f\"Saving normalized entropy histograms at `{estimation_volume.voxel_normalized_entropy_histograms_path=}`\")\n",
    "np.save(\n",
    "    estimation_volume.voxel_normalized_entropy_histograms_path,\n",
    "    entropy_histograms\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "logger.debug(f\"Saving figure {(fig_name := 'normalized-entropy-histogram-per-label.png')=}\")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    1, 1, figsize=(2 * (sz := 6), sz), dpi=(dpi := 200),\n",
    ")\n",
    "xlims = (-.05, 1.05)\n",
    "ylims_10pow = (-5, 0)\n",
    "ylims = tuple(10 ** p for p in ylims_10pow)\n",
    "\n",
    "for label_idx in labels_idx:\n",
    "    logger.debug(f\"Plotting histogram for {label_idx=}\")\n",
    "    \n",
    "    ax.step(\n",
    "        np.concatenate([bins, [1.001]]),\n",
    "        np.concatenate([[0], entropy_histograms[label_idx], [0]]),\n",
    "        label=f\"voxels '{labels_names[label_idx]}' (idx={label_idx}) (#voxels: {humanize.intcomma(labels_counts[label_idx])})\",\n",
    "        linewidth=1.5\n",
    "    )\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim(ylims[0], 1.5 * ylims[1])\n",
    "    ax.set_yticks(np.logspace(ylims_10pow[0], ylims_10pow[1], ylims_10pow[1] - ylims_10pow[0] + 1))\n",
    "    ax.set_ylabel(\n",
    "        f\"Proportion of voxels in *log scale* \\n\"\n",
    "        f\"log10(#voxels in bin / #voxels of '{(label_name := labels_names[label_idx])}')\"\n",
    "    )\n",
    "    \n",
    "    ax.set_xlim(*xlims)\n",
    "    ax.set_xticks(np.linspace(0, 1, 11))\n",
    "    ax.set_xlabel(\"Normalized entropy\\nbins of 0.01 = 1% width\")\n",
    "\n",
    "    ax.tick_params(axis='y', left=True, right=True, labelleft=True, labelright=True, which=\"both\")    \n",
    "    ax.grid(True, axis='y', which='major', ls='--')    \n",
    "    \n",
    "    ax.set_title(\n",
    "        f\"Normalized entropy (NE) histogram\\n\"\n",
    "        f\"NE = - (SUM_i P[x_i] * log(P[x_i])) / MAX_NE\" \n",
    "    )\n",
    "    ax.legend(loc=\"upper center\", fontsize=\"small\", framealpha=1)\n",
    "\n",
    "# logger.info(f\"Saving figure {(figname := display.title + '.png')=}\")\n",
    "# display.fig_.savefig(\n",
    "#     fname=figs_dir / figname,\n",
    "#     format=\"png\",\n",
    "#     metadata=display.metadata,\n",
    "# )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Probabilities plane (2D)\n",
    "\n",
    "It is possible to visualize the probabilities on a plane because \n",
    "\n",
    "$$\\sum_{i = 1}^{n\\_classes} p_i = 1$$\n",
    "\n",
    "So, when $n\\_classes = 3$, we actually have 2 degrees of freedom. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_state = np.random.RandomState(32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "probas_volume_reshaped = probas_volume.reshape(-1, 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "logger.info(\n",
    "    f\"Saving figure of probabilities scattered on a plane {(fig_name := f'probabilities-scatter.png')=}\"\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    n_rows := 1, \n",
    "    n_cols := 1, \n",
    "    figsize=(n_cols * (sz := 16), n_rows * sz), \n",
    "    dpi=(dpi := 150),\n",
    ")\n",
    "\n",
    "x_label, y_label, z_label = labels_idx\n",
    "\n",
    "logger.debug(f\"{(x_label, y_label, z_label)=}\")\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "argmax_points = [\n",
    "    xmax_points := np.array([[.5, 0], [1, 0], [.5, .5], [1./3, 1./3]]),\n",
    "    ymax_points := np.vstack([xmax_points[:, 1], xmax_points[:, 0]]).T,\n",
    "    zmax_points := np.array([[0, 0], [0, .5], [1./3, 1./3], [.5, 0]]),\n",
    "]\n",
    "\n",
    "for points, c in zip(argmax_points, colors):\n",
    "    ax.add_patch(matplotlib.patches.Polygon(points, color=c, alpha=.1))\n",
    "\n",
    "n_per_label = int(1e3)\n",
    "logger.debug(f\"{n_per_label=}\")\n",
    "\n",
    "ax.set_xlim(0, 1 + (margin := .00))\n",
    "ax.set_ylim(0, 1 + margin)\n",
    "\n",
    "ticks = np.linspace(0, 1, 11)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Voxel probabilities scatter with {humanize.intcomma(n_per_label)} voxels per label\\n\"\n",
    "    \"background colors indicate the class of max probability\",\n",
    "    fontsize='x-large'\n",
    ") \n",
    "ax.set_xlabel(f\"Probability of '{labels_names[x_label]}' (idx={x_label})\", fontsize='x-large')\n",
    "ax.set_ylabel(f\"Probability of '{labels_names[y_label]}' (idx={y_label})\", fontsize='x-large')\n",
    "\n",
    "for third_proba in ticks[1:-1]:\n",
    "    ax.plot([0, third_proba], [third_proba, 0], c='gray', ls='-.', linewidth=.35)\n",
    "    ax.text(third_proba / 2, third_proba / 2, f\"{1. - third_proba:.1f}\", c='gray', rotation=-45, rotation_mode='anchor', fontsize='small')\n",
    "ax.plot([0, 1], [1, 0], c='k', ls='--', linewidth=.75)\n",
    "ax.text((p := .4) + .01, 1 - p, f\"Probability of '{labels_names[z_label]}' (idx={z_label}) = 0.0\", c='k', rotation=-44.5, rotation_mode='anchor', fontsize='x-large')\n",
    "\n",
    "for label_idx, c, label_name in zip(labels_idx, colors, labels_names):\n",
    "    logger.debug(f\"{label_idx=}\")\n",
    "    sample = probas_volume_reshaped[labels_volume.ravel() == label_idx, :]\n",
    "    idx = random_state.choice(\n",
    "        np.arange(0, len(sample)), size=n_per_label, replace=False\n",
    "    )\n",
    "    sample = sample[idx, :]\n",
    "    logger.debug(f\"{label_idx=}  {len(sample)=}\")\n",
    "    ax.scatter(\n",
    "        sample[:, x_label], sample[:, y_label],\n",
    "        c=c, marker='.', label=label_name\n",
    "    )\n",
    "    \n",
    "ax.legend(loc='upper right', fontsize='xx-large')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del probas_volume_reshaped"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
