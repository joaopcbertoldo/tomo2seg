{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import copy\n",
    "import dataclasses\n",
    "import functools\n",
    "import gc\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import pprint as pprint_module\n",
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import humanize\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from cnn_segm import keras_custom_loss\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import RandomState\n",
    "from progressbar import progressbar as pbar\n",
    "from pymicro.file import file_utils\n",
    "import socket\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tomo2seg.process import ProcessVolumeArgs as Args, reduce_dimensions \n",
    "from tomo2seg import viz\n",
    "from tomo2seg.data import EstimationVolume\n",
    "from tomo2seg.data import Volume\n",
    "from tomo2seg.logger import add_file_handler as logger_add_file_handler\n",
    "from tomo2seg.logger import logger\n",
    "from tomo2seg.model import Model as Tomo2SegModel\n",
    "from tomo2seg import utils as tomo2seg_utils\n",
    "from tomo2seg import slackme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this registers a custom exception handler for the whole current notebook\n",
    "get_ipython().set_custom_exc((Exception,), slackme.custom_exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet2d\n",
      "unet2d-sep\n",
      "unet2halfd\n",
      "unet2halfd-sep\n",
      "unet3d\n",
      "unet2d.crop112-f16.fold000.1607-533-765\n",
      "unet2d.crop48-f16.fold000.1607-530-580\n",
      "unet2d.vanilla01-f04.fold000.1606-326-760\n",
      "unet2d.vanilla01-f08.fold000.1606-377-881\n",
      "unet2d.vanilla01-f16.fold000.1606-391-447\n",
      "unet2d.vanilla02-f04.fold000.1606-419-966\n",
      "unet2d.vanilla02-f08.fold000.1606-431-664\n",
      "unet2d.vanilla02-f16.fold000.1606-461-820\n",
      "unet2d.vanilla03-f16.fold000.1606-505-109\n",
      "unet2halfd.vanilla00-f02.fold000.1606-654-198\n",
      "unet2halfd.vanilla00-f04.fold000.1606-680-413\n",
      "unet2halfd.vanilla03-f16.fold000.1606-683-705\n",
      "unet3d.crop112-f12.fold000.1607-466-349\n",
      "unet3d.crop96-f08.fold000.1607-109-265\n",
      "unet3d.vanilla03-f08.fold000.1606-842-005\n",
      "unet3d.vanilla03-f16.fold000.1606-750-939\n",
      "unet2halfd-sep.vanilla03-f16.fold000.1606-729-672\n",
      "unet2d-sep.vanilla03-f16.fold000.1606-575-226\n"
     ]
    }
   ],
   "source": [
    "!find ../data/models -maxdepth 1 -type d -exec ls \"{}\" \\; | grep -v hdf5 \n",
    "# print all available models' names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomo2seg.datasets import (\n",
    "    VOLUME_COMPOSITE_V1 as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_COMPOSITE_V1_REDUCED as VOLUME_NAME_VERSION,\n",
    "#     VOLUME_COMPOSITE_NEIGHBOUR as VOLUME_NAME_VERSION,    \n",
    "#     VOLUME_COMPOSITE_FLEX as VOLUME_NAME_VERSION,    \n",
    "#     VOLUME_COMPOSITE_BIAXE as VOLUME_NAME_VERSION,    \n",
    ")\n",
    "\n",
    "\n",
    "args = Args(\n",
    "    model_name = \"unet2halfd.vanilla03-f16.fold000.1606-683-705\",\n",
    "    model_type = Args.ModelType.input2halfd, \n",
    "    \n",
    "    model_shape_min_multiple_requirement = 16,\n",
    "    \n",
    "    volume_name=VOLUME_NAME_VERSION[0], \n",
    "    volume_version=VOLUME_NAME_VERSION[1], \n",
    "    \n",
    "#     partition_alias=None,\n",
    "    partition_alias=\"test\",\n",
    "    \n",
    "    cropping_strategy=Args.CroppingStrategy.maximum_size_reduced_overlap, \n",
    "    aggregation_strategy=Args.AggregationStrategy.average_probabilities, \n",
    "    \n",
    "    runid = 1607616782,  # default is time.time()\n",
    "    probabilities_dtype = np.float16,\n",
    "    \n",
    "    opts=Args.ProcessVolumeOpts(\n",
    "        save_probas_by_class = False,\n",
    "        debug__save_figs = True,\n",
    "        save_logs=True,\n",
    "        override_batch_size=None,\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build `tomo2seg` objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{logger.py:add_file_handler:020}::[2020-12-11::12:29:07.989]\n",
      "Added a new file handler to the logger. logspath='/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.exec.log'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tomo2seg_model = Tomo2SegModel.build_from_model_name(args.model_name)\n",
    "\n",
    "volume = Volume.with_check(\n",
    "    name=args.volume_name, \n",
    "    version=args.volume_version\n",
    ")\n",
    "\n",
    "partition = volume[args.partition_alias] if args.partition_alias is not None else None\n",
    "\n",
    "estimation_volume = EstimationVolume.from_objects(\n",
    "    volume=volume, \n",
    "    model=tomo2seg_model, \n",
    "    set_partition=partition,\n",
    "    runid=args.runid,\n",
    ")\n",
    "\n",
    "if args.opts.save_logs:\n",
    "    logger_add_file_handler(logger, estimation_volume.exec_log_path)\n",
    "\n",
    "# this is informal metadata for human use\n",
    "estimation_volume[\"args\"] = dataclasses.asdict(args)\n",
    "estimation_volume[\"hostname\"] = hostname = socket.gethostname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-8-4136882eb0f2>:<module>:001}::[2020-12-11::12:29:11.561]\n",
      "args\n",
      "{   'aggregation_strategy': <AggregationStrategy.average_probabilities: 0>,\n",
      "    'cropping_strategy': <CroppingStrategy.maximum_size_reduced_overlap: 1>,\n",
      "    'model_name': 'unet2halfd.vanilla03-f16.fold000.1606-683-705',\n",
      "    'model_shape_min_multiple_requirement': 16,\n",
      "    'model_type': <ModelType.input2halfd: 1>,\n",
      "    'opts': {   'debug__save_figs': True,\n",
      "                'override_batch_size': None,\n",
      "                'save_logs': True,\n",
      "                'save_probas_by_class': False},\n",
      "    'partition_alias': 'test',\n",
      "    'probabilities_dtype': <class 'numpy.float16'>,\n",
      "    'random_state_seed': 42,\n",
      "    'runid': 1607616782,\n",
      "    'volume_name': 'PA66GF30',\n",
      "    'volume_version': 'v1'}\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-8-4136882eb0f2>:<module>:002}::[2020-12-11::12:29:11.562]\n",
      "estimation_volume=EstimationVolume(volume_fullname='PA66GF30.v1', model_name='unet2halfd.vanilla03-f16.fold000.1606-683-705', runid=1607616782, partition=SetPartition(x_range=(0, 1300), y_range=(0, 1040), z_range=(1300, 1600), alias='test'))\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-8-4136882eb0f2>:<module>:003}::[2020-12-11::12:29:11.563]\n",
      "estimation_volume.fullname='vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-8-4136882eb0f2>:<module>:004}::[2020-12-11::12:29:11.564]\n",
      "estimation_volume.dir=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"args\\n{pprint_module.PrettyPrinter(indent=4, compact=False).pformat(dataclasses.asdict(args))}\")\n",
    "logger.info(f\"{estimation_volume=}\")\n",
    "logger.info(f\"{estimation_volume.fullname=}\")\n",
    "logger.info(f\"{estimation_volume.dir=}\")\n",
    "            \n",
    "logger.debug(f\"{volume=}\")\n",
    "logger.debug(f\"{partition=}\")\n",
    "logger.debug(f\"{tomo2seg_model=}\")\n",
    "logger.debug(f\"{tomo2seg_model.name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:29:25.040]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-9-086a3ca43fdc>:<module>:008}::[2020-12-11::12:29:25.080]\n",
      "tf_version='2.2.0'\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:29:25.082]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-9-086a3ca43fdc>:<module>:011}::[2020-12-11::12:29:25.106]\n",
      "Num GPUs Available: 1\n",
      "This should be:\n",
      "\t2 on R790-TOMO\n",
      "\t1 on akela\n",
      "\t1 on hathi\n",
      "\t1 on krilin\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-9-086a3ca43fdc>:<module>:013}::[2020-12-11::12:29:25.211]\n",
      "physical GPU devices:\n",
      "\tPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "logical GPU devices:\n",
      "\tLogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.setLevel(logging.DEBUG)\n",
    "random_state = np.random.RandomState(args.random_state_seed)\n",
    "\n",
    "n_gpus = len(tf.config.list_physical_devices('GPU'))\n",
    "estimation_volume[\"n_gpus\"] = n_gpus\n",
    "    \n",
    "tf_version = tf.__version__\n",
    "logger.info(f\"{tf_version=}\")\n",
    "estimation_volume[\"tf_version\"] = tf_version\n",
    "\n",
    "logger.info(f\"Num GPUs Available: {n_gpus}\\nThis should be:\\n\\t\" + '\\n\\t'.join(['2 on R790-TOMO', '1 on akela', '1 on hathi', '1 on krilin']))\n",
    "\n",
    "logger.debug(\n",
    "    \"physical GPU devices:\\n\\t\" + \"\\n\\t\".join(map(str, tf.config.list_physical_devices('GPU'))) + \"\\n\" +\n",
    "    \"logical GPU devices:\\n\\t\" + \"\\n\\t\".join(map(str, tf.config.list_logical_devices('GPU'))) \n",
    ")\n",
    "\n",
    "# xla auto-clustering optimization (see: https://www.tensorflow.org/xla#auto-clustering)\n",
    "# this seems to break the training\n",
    "tf.config.optimizer.set_jit(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figure out the gpu's limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-10-fbabdd13e1dd>:<module>:024}::[2020-12-11::12:29:26.779]\n",
      "hostname='akela.materiaux.ensmp.fr'\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-10-fbabdd13e1dd>:<module>:025}::[2020-12-11::12:29:26.780]\n",
      "MAX_INTERNAL_NVOXELS=133632000 (133,632,000)\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:29:26.782]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# an estimate of how much the gpu can take\n",
    "MAX_INTERNAL_NVOXELS = max(\n",
    "    # seen cases (this is just empirical)\n",
    "    # batch_size * internal_multiplier_factor * input_nvoxels:\n",
    "    4 * (8 * 6) * (96**3),\n",
    "    8 * (16 * 6) * (320**2),  \n",
    "    3 * (16 * 6) * (800 * 928),\n",
    ")\n",
    "\n",
    "# this factor is specific to the gpu's memory size\n",
    "# to correct the fact that the max above was on an 8Gb-gpu\n",
    "known_hosts_factors = {\n",
    "    \"R7920-tomo\": 1,\n",
    "    \"akela.materiaux.ensmp.fr\": 5/8,\n",
    "    \"hathi.materiaux.ensmp.fr\": 5/8,\n",
    "    \"krilin.materiaux.ensmp.fr\": 5/8,\n",
    "}\n",
    "\n",
    "if n_gpus > 0 and hostname not in known_hosts_factors:\n",
    "    raise Exception(f\"Unkown {hostname=} with {n_gpus=} available. Please tell me how big the memory is relative to 8Gb.\")\n",
    "    \n",
    "MAX_INTERNAL_NVOXELS = int(known_hosts_factors[hostname] * MAX_INTERNAL_NVOXELS)\n",
    "\n",
    "logger.info(f\"{hostname=}\")\n",
    "logger.info(f\"{MAX_INTERNAL_NVOXELS=} ({humanize.intcomma(MAX_INTERNAL_NVOXELS)})\")\n",
    "\n",
    "estimation_volume[\"MAX_INTERNAL_NVOXELS\"] = MAX_INTERNAL_NVOXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-11-d8ff6289d185>:<module>:003}::[2020-12-11::12:29:27.327]\n",
      "figs_dir=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/debug_figs')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.opts.debug__save_figs:\n",
    "    figs_dir = estimation_volume.dir / \"debug_figs\"\n",
    "    logger.debug(f\"{figs_dir=}\")\n",
    "    figs_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `tf.distribute.OneDeviceStrategy` \n",
    "\n",
    "first just open the model to see that everything goes right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-12-de580b709a33>:<module>:003}::[2020-12-11::12:29:28.636]\n",
      "one_device=<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy object at 0x7f7157430820>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get a distribution strategy to use both gpus (see https://www.tensorflow.org/guide/distributed_training)\n",
    "one_device = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\" if n_gpus > 0 else \"/cpu:0\")\n",
    "logger.debug(f\"{one_device=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:<module>:050}::[2020-12-11::12:29:29.535]\n",
      "Loading model with OneDeviceStrategy.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:003}::[2020-12-11::12:29:29.536]\n",
      "Loading model from autosaved file: unet2halfd.vanilla03-f16.fold000.1606-683-705.autosaved.hdf5\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:010}::[2020-12-11::12:29:34.083]\n",
      "Changing the model's input type to accept any size of crop.\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:016}::[2020-12-11::12:29:34.084]\n",
      "input_n_channels=5\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:029}::[2020-12-11::12:29:34.085]\n",
      "anysize_target_shape=[None, None, 5]\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:035}::[2020-12-11::12:29:34.087]\n",
      "anysize_input=<tf.Tensor 'input_any_image_size:0' shape=(None, None, None, 5) dtype=float32>\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:043}::[2020-12-11::12:29:34.088]\n",
      "Starting model compilation\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-13-f4c3e346eb9c>:get_model:045}::[2020-12-11::12:29:34.099]\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    \n",
    "    logger.info(f\"Loading model from autosaved file: {tomo2seg_model.autosaved_model_path.name}\")\n",
    "    \n",
    "    model = tf.keras.models.load_model(\n",
    "        tomo2seg_model.autosaved_model_path_str,\n",
    "        compile=False\n",
    "    )\n",
    "    \n",
    "    logger.debug(\"Changing the model's input type to accept any size of crop.\")\n",
    "    \n",
    "    in_ = model.layers[0]\n",
    "    in_shape = in_.input_shape[0]\n",
    "    input_n_channels = in_shape[-1]\n",
    "\n",
    "    logger.debug(f\"{input_n_channels=}\")\n",
    "    \n",
    "    if input_n_channels > 1:\n",
    "        \n",
    "        if args.model_type == Args.ModelType.input2halfd:\n",
    "            if len(in_shape) != 4:\n",
    "                raise f\"len({in_shape=}) > 4, so this model must be multi-channel. Not supported yet...\"\n",
    "        else:\n",
    "            raise NotImplementedError(f\"{input_n_channels=} > 1\")\n",
    "    \n",
    "    # make it capable of getting any dimension in the input\n",
    "    # \"-2\" = 1 for the batch size, 1 for the nb.channels\n",
    "    anysize_target_shape = (len(in_shape) - 2) * [None] + [input_n_channels] \n",
    "    logger.debug(f\"{anysize_target_shape=}\")\n",
    "    \n",
    "    anysize_input = layers.Input(\n",
    "        shape=anysize_target_shape,\n",
    "        name=\"input_any_image_size\"\n",
    "    )\n",
    "    logger.debug(f\"{anysize_input=}\")\n",
    "    \n",
    "    model.layers[0] = anysize_input\n",
    "    \n",
    "    # this doesn't really matter bc this script will not fit the model\n",
    "    optimizer = optimizers.Adam()\n",
    "    loss_func = keras_custom_loss.jaccard2_loss\n",
    "\n",
    "    logger.debug(\"Starting model compilation\")\n",
    "    model.compile(loss=loss_func, optimizer=optimizer)\n",
    "    logger.debug(\"Done!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "with one_device.scope():\n",
    "    logger.info(f\"Loading model with {one_device.__class__.__name__}.\")\n",
    "    model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-14-1898b8fbbba1>:<module>:005}::[2020-12-11::12:29:34.133]\n",
      "Loading data from disk at file: PA66GF30.v1.raw\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-14-1898b8fbbba1>:<module>:006}::[2020-12-11::12:29:34.134]\n",
      "volume.data_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/PA66GF30.v1/PA66GF30.v1.raw')\n",
      "\n",
      "data type is uint8\n",
      "volume size is 1300 x 1040 x 1900\n",
      "reading volume... from byte 0\n",
      "DEBUG::tomo2seg::{<ipython-input-14-1898b8fbbba1>:<module>:016}::[2020-12-11::12:29:42.554]\n",
      "data_volume.shape=(1300, 1040, 1900)\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-14-1898b8fbbba1>:<module>:020}::[2020-12-11::12:29:42.556]\n",
      "Cutting data with partition.alias='test'\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-14-1898b8fbbba1>:<module>:021}::[2020-12-11::12:29:42.556]\n",
      "partition=SetPartition(x_range=(0, 1300), y_range=(0, 1040), z_range=(1300, 1600), alias='test')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _read_raw(path_: Path, volume_: Volume): \n",
    "    # from pymicro\n",
    "    return \n",
    "\n",
    "logger.info(f\"Loading data from disk at file: {volume.data_path.name}\")\n",
    "logger.debug(f\"{volume.data_path=}\")\n",
    "\n",
    "data_volume = file_utils.HST_read(\n",
    "    str(volume.data_path),  # it doesn't accept paths...\n",
    "    autoparse_filename=False,  # the file names are not properly formatted\n",
    "    data_type=volume.metadata.dtype,\n",
    "    dims=volume.metadata.dimensions,\n",
    "    verbose=True,\n",
    ") / 255  # normalize\n",
    "\n",
    "logger.debug(f\"{data_volume.shape=}\")\n",
    "\n",
    "if partition is not None:\n",
    "    \n",
    "    logger.info(f\"Cutting data with {partition.alias=}\")\n",
    "    logger.debug(f\"{partition=}\")\n",
    "    \n",
    "    data_volume = partition.get_volume_partition(data_volume)\n",
    "\n",
    "else:\n",
    "    logger.debug(f\"No partition. The whole volume will be processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify the data if necessary \n",
    "\n",
    "mostly the 2halfd..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING::tomo2seg::{<ipython-input-15-a4c94338b1d3>:<module>:009}::[2020-12-11::12:29:49.219]\n",
      "Modifying the data to add a 'reflect' half padding to the data. Only z-layers 2.5d models are supported!\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-a4c94338b1d3>:<module>:017}::[2020-12-11::12:29:49.221]\n",
      "nlayers_2halfd=5\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-a4c94338b1d3>:<module>:018}::[2020-12-11::12:29:49.222]\n",
      "predicted_layer_idx_2halfd=2\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-a4c94338b1d3>:<module>:019}::[2020-12-11::12:29:49.223]\n",
      "slice_2halfd_data_predicted_layer=slice(2, 3, None)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-a4c94338b1d3>:<module>:025}::[2020-12-11::12:29:49.224]\n",
      "half_pad=2\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-15-a4c94338b1d3>:<module>:033}::[2020-12-11::12:29:50.947]\n",
      "data_volume.shape=(1300, 1040, 304)\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:29:50.948]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:29:50.973]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.model_type == Args.ModelType.input2halfd:\n",
    "\n",
    "    try:\n",
    "        # this is to prevent running the padding twice in the notebook\n",
    "        half_pad\n",
    "\n",
    "    except NameError:\n",
    "\n",
    "            logger.warning(\"Modifying the data to add a 'reflect' half padding to the data. Only z-layers 2.5d models are supported!\")\n",
    "\n",
    "            nlayers_2halfd = model.layers[0].input_shape[0][-1]\n",
    "            \n",
    "            predicted_layer_idx_2halfd = nlayers_2halfd // 2\n",
    "            \n",
    "            slice_2halfd_data_predicted_layer = slice(predicted_layer_idx_2halfd, predicted_layer_idx_2halfd + 1)\n",
    "\n",
    "            logger.debug(f\"{nlayers_2halfd=}\")\n",
    "            logger.debug(f\"{predicted_layer_idx_2halfd=}\")\n",
    "            logger.debug(f\"{slice_2halfd_data_predicted_layer=}\")\n",
    "\n",
    "            assert nlayers_2halfd % 2 == 1, f\"{nlayers_2halfd=} should be an odd number\"\n",
    "\n",
    "            half_pad = (nlayers_2halfd - 1) // 2\n",
    "\n",
    "            logger.debug(f\"{half_pad=}\")\n",
    "\n",
    "            data_volume = np.pad(\n",
    "                data_volume, \n",
    "                pad_width=((0, 0), (0, 0), (half_pad, half_pad)),\n",
    "                mode=\"reflect\",\n",
    "            )\n",
    "\n",
    "            logger.debug(f\"{data_volume.shape=}\")    \n",
    "            estimation_volume[\"volume_is_padded\"] = True\n",
    "            estimation_volume[\"half_pad\"] = half_pad\n",
    "            \n",
    "    else:\n",
    "        logger.debug(\"Padding already applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-16-168e0ad98268>:<module>:002}::[2020-12-11::12:29:56.954]\n",
      "volume_shape=(1300, 1040, 304)\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-16-168e0ad98268>:<module>:003}::[2020-12-11::12:29:56.955]\n",
      "data_volume.size=411008000  (411.0 million)\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:29:56.957]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "volume_shape = data_volume.shape\n",
    "logger.info(f\"{volume_shape=}\")\n",
    "logger.info(f\"{data_volume.size=}  ({humanize.intword(data_volume.size)})\")\n",
    "estimation_volume[\"volume_shape\"] = volume_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many voxels the gpus can take in a single batch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-17-0db215765072>:<module>:001}::[2020-12-11::12:30:01.919]\n",
      "args.model_shape_min_multiple_requirement=16\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-17-0db215765072>:<module>:002}::[2020-12-11::12:30:01.920]\n",
      "MAX_INTERNAL_NVOXELS=133632000 (133,632,000)\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:023}::[2020-12-11::12:30:01.921]\n",
      "input_layer=<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f7157178100>\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:029}::[2020-12-11::12:30:01.922]\n",
      "input_nvoxels=103680\n",
      "\n",
      "DEBUG::tomo2seg::{utils.py:get_model_internal_nvoxel_factor:041}::[2020-12-11::12:30:01.923]\n",
      "max_internal_nvoxels=1990656 (1,990,656)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-17-0db215765072>:<module>:006}::[2020-12-11::12:30:01.923]\n",
      "internal_nvoxel_factor=20\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-17-0db215765072>:<module>:010}::[2020-12-11::12:30:01.924]\n",
      "max_batch_nvoxels=6681600 (6,681,600)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"{args.model_shape_min_multiple_requirement=}\")\n",
    "logger.info(f\"{MAX_INTERNAL_NVOXELS=} ({humanize.intcomma(MAX_INTERNAL_NVOXELS)})\")\n",
    "\n",
    "internal_nvoxel_factor = tomo2seg_utils.get_model_internal_nvoxel_factor(model)\n",
    "\n",
    "logger.debug(f\"{internal_nvoxel_factor=}\")\n",
    "\n",
    "max_batch_nvoxels = int(np.floor(MAX_INTERNAL_NVOXELS / internal_nvoxel_factor))\n",
    "\n",
    "logger.info(f\"{max_batch_nvoxels=} ({humanize.intcomma(max_batch_nvoxels)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "figure out the crop shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-18-5c16f5862093>:<module>:001}::[2020-12-11::12:30:35.455]\n",
      "Using args.cropping_strategy=maximum_size_reduced_overlap to find a suitable crop size.\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-18-5c16f5862093>:<module>:027}::[2020-12-11::12:30:35.456]\n",
      "the max overlap in each direction will be (28, 0, 0)\n",
      "\n",
      "DEBUG::tomo2seg::{<ipython-input-18-5c16f5862093>:<module>:032}::[2020-12-11::12:30:35.457]\n",
      "crop_dims_multiple=(656, 1040, 304)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Using args.cropping_strategy={args.cropping_strategy.name} to find a suitable crop size.\")\n",
    "\n",
    "if args.cropping_strategy == Args.CroppingStrategy.maximum_size:\n",
    "    \n",
    "    crop_dims_multiple = process.get_largest_crop_multiple(\n",
    "        volume_shape, \n",
    "        multiple_of=args.model_shape_min_multiple_requirement\n",
    "    )\n",
    "\n",
    "elif args.cropping_strategy == Args.CroppingStrategy.maximum_size_reduced_overlap:\n",
    "    \n",
    "    # it's not necessarily the real minimum, just an easy way to get a big crop with less overlap\n",
    "    # get the largest multiple of the requirement above the dimension size / 2\n",
    "    # that will give a max overlap of 2 * MULTIPLE_REQUIREMENT - 1\n",
    "    # e.g. with MULTIPLE_REQUIREMENT = 16, the maximum overlap is 31\n",
    "    _mult = args.model_shape_min_multiple_requirement\n",
    "    crop_dims_multiple = tuple(\n",
    "        (1 + int((dim / 2) // _mult)) * _mult if dim % _mult != 0 else\n",
    "        dim\n",
    "        for dim in volume_shape\n",
    "    )\n",
    "    \n",
    "    def max_overlap(size):\n",
    "        overlap = int(2 * _mult - size % _mult)\n",
    "        return overlap if overlap < 32 else 0 \n",
    "    \n",
    "    logger.info(f\"the max overlap in each direction will be {tuple(max_overlap(s) for s in volume_shape)}\")\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"{args.cropping_strategy=}\")\n",
    "\n",
    "logger.debug(f\"{crop_dims_multiple=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adjust the crop dimension if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-19-5229a0be0602>:<module>:022}::[2020-12-11::12:30:37.340]\n",
      "ideal crop_shape=(656, 1040, 5) for args.model_type=<ModelType.input2halfd: 1> now let's see if the maximum number of voxels is ok...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it has to be multiple of 16 because of the 4 cascaded 2x2-strided 2x2-downsamplings in u-net\n",
    "if args.model_type == Args.ModelType.input2d:\n",
    "    crop_shape = (\n",
    "        crop_dims_multiple[0],\n",
    "        crop_dims_multiple[1],\n",
    "        1,\n",
    "    )\n",
    "\n",
    "elif args.model_type == Args.ModelType.input2halfd:\n",
    "    crop_shape = (\n",
    "        crop_dims_multiple[0],\n",
    "        crop_dims_multiple[1],\n",
    "        nlayers_2halfd,\n",
    "    )\n",
    "    \n",
    "elif args.model_type == Args.ModelType.input3d:\n",
    "    crop_shape = crop_dims_multiple\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"{args.model_type=}\")\n",
    "\n",
    "logger.debug(f\"ideal {crop_shape=} for {args.model_type=} now let's see if the maximum number of voxels is ok...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO::tomo2seg::{<ipython-input-20-9637e8d427a2>:<module>:007}::[2020-12-11::12:30:37.397]\n",
      "crop_shape=(656, 1040, 5) \n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-20-9637e8d427a2>:<module>:011}::[2020-12-11::12:30:37.398]\n",
      "crop_nvoxels=3411200 (3,411,200)\n",
      "\n",
      "INFO::tomo2seg::{<ipython-input-20-9637e8d427a2>:<module>:015}::[2020-12-11::12:30:37.399]\n",
      "max_batch_size_per_gpu=1\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:30:37.400]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n",
      "DEBUG::tomo2seg::{data.py:__setitem__:336}::[2020-12-11::12:30:37.434]\n",
      "Writing to file self.metadata_path=PosixPath('/home/users/jcasagrande/projects/tomo2seg/data/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782/vol=PA66GF30.v1.set=test.model=unet2halfd.vanilla03-f16.fold000.1606-683-705.runid=1607-616-782.metadata.yml').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crop_shape = reduce_dimensions(\n",
    "    crop_shape,\n",
    "    max_nvoxels=max_batch_nvoxels,\n",
    "    multiple_of=args.model_shape_min_multiple_requirement,\n",
    ")\n",
    "    \n",
    "logger.info(f\"{crop_shape=} \")\n",
    "\n",
    "crop_nvoxels = functools.reduce(operator.mul, crop_shape)\n",
    "\n",
    "logger.info(f\"{crop_nvoxels=} ({humanize.intcomma(crop_nvoxels)})\")\n",
    "\n",
    "max_batch_size_per_gpu = int(np.floor(max_batch_nvoxels / crop_nvoxels))\n",
    "\n",
    "logger.info(f\"{max_batch_size_per_gpu=}\")\n",
    "\n",
    "estimation_volume[\"crop_shape\"] = crop_shape\n",
    "estimation_volume[\"crop_nvoxels\"] = crop_nvoxels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps and coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG::tomo2seg::{<ipython-input-21-36de51d675f1>:<module>:006}::[2020-12-11::12:33:23.994]\n",
      "n_steps=(2, 1, 61)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_steps = tuple(\n",
    "    int(np.ceil(vol_dim / crop_dim))\n",
    "    for vol_dim, crop_dim in zip(volume_shape, crop_shape)\n",
    ")\n",
    "\n",
    "logger.debug(f\"{n_steps=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_iterator(n_steps_):\n",
    "    assert len(n_steps_) == 3\n",
    "    return itertools.product(*(range(n_steps_[dim]) for dim in range(3)))\n",
    "\n",
    "get_ijk_iterator = functools.partial(\n",
    "    get_coordinates_iterator, copy.copy(n_steps)\n",
    ")\n",
    "\n",
    "get_kji_iterator = functools.partial(\n",
    "    get_coordinates_iterator, tuple(reversed(n_steps))\n",
    ")\n",
    "\n",
    "# coordinates (xs, ys, and zs) of the front upper left corners of the crops\n",
    "x0s, y0s, z0s = tuple(\n",
    "    tuple(map(\n",
    "        int, \n",
    "        np.linspace(0, vol_dim - crop_dim, n)\n",
    "    ))\n",
    "    for vol_dim, crop_dim, n in zip(volume_shape, crop_shape, n_steps)\n",
    ")\n",
    "logger.debug(f\"{min(x0s)=}, {max(x0s)=}, {len(x0s)=}\")\n",
    "logger.debug(f\"{min(y0s)=}, {max(y0s)=}, {len(y0s)=}\")\n",
    "logger.debug(f\"{min(z0s)=}, {max(z0s)=}, {len(z0s)=}\")\n",
    "\n",
    "estimation_volume[\"n_steps\"] = n_steps\n",
    "estimation_volume[\"x0s\"] = x0s\n",
    "estimation_volume[\"y0s\"] = y0s\n",
    "estimation_volume[\"z0s\"] = z0s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crops coordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"Generating the crop coordinates.\")\n",
    "\n",
    "crops_coordinates = np.array(\n",
    "    [\n",
    "        (\n",
    "            (x0, x0 + crop_shape[0]), \n",
    "            (y0, y0 + crop_shape[1]),\n",
    "            (z0, z0 + crop_shape[2]),\n",
    "        )\n",
    "        for x0, y0, z0 in itertools.product(x0s, y0s, z0s)\n",
    "    ], \n",
    "    dtype=tuple\n",
    ").reshape(len(x0s), len(y0s), len(z0s), 3, 2).astype(int)  # 3 = nb of dimenstions, 2 = (start, end)\n",
    "\n",
    "logger.debug(f\"{crops_coordinates.shape=}\")\n",
    "\n",
    "# 'F' reshapes with x varying fastest and z slowest\n",
    "crops_coordinates_sequential = crops_coordinates.reshape(-1, 3, 2, order='F')  \n",
    "\n",
    "logger.debug(f\"{crops_coordinates_sequential.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orthogonal slices plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.opts.debug__save_figs:\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(sz := 15, sz), dpi=120)\n",
    "    fig.set_tight_layout(True)\n",
    "    \n",
    "    display = viz.OrthogonalSlicesDisplay(\n",
    "        volume=data_volume,\n",
    "        volume_name=volume.fullname,\n",
    "    ).plot(axs=axs,)\n",
    "    \n",
    "    logger.info(f\"Saving figure {(figname := display.title + '.png')=}\")\n",
    "    display.fig_.savefig(\n",
    "        fname=figs_dir / figname,\n",
    "        dpi=200, format=\"png\",\n",
    "        metadata=display.metadata,\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_ijk = (0, 0, 0)\n",
    "i, j, k = crop_ijk\n",
    "crop_coords = crops_coordinates[i, j, k]\n",
    "\n",
    "logger.info(f\"Segmenting one crop for debug {crop_ijk=}\")\n",
    "\n",
    "crop_data = data_volume[tuple(slice(*coords_) for coords_ in crop_coords)]\n",
    "    \n",
    "logger.debug(f\"{crop_data.shape=}\")\n",
    "\n",
    "# [model] - i call it with a first crop bc if something goes wrong then the error\n",
    "# will appear here instead of in a loop\n",
    "\n",
    "modelin_target_shape = (1, crop_shape[0], crop_shape[1], crop_shape[2], 1)\n",
    "logger.debug(f\"{modelin_target_shape=}\")\n",
    "\n",
    "if args.model_type == Args.ModelType.input2halfd:\n",
    "    crop_probas_target_shape = list(crop_shape[:2]) + [1] + [n_classes]\n",
    "    \n",
    "else:\n",
    "    crop_probas_target_shape = list(crop_shape) + [n_classes]\n",
    "    \n",
    "logger.debug(f\"{crop_probas_target_shape=}\")\n",
    "\n",
    "# modelin\n",
    "modelin = crop_data.reshape(modelin_target_shape) \n",
    "\n",
    "# modelout\n",
    "modelout = model.predict(\n",
    "    modelin, \n",
    "    batch_size=1,\n",
    "    steps=1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{modelout.shape=}\")\n",
    "\n",
    "n_classes = modelout.shape[-1]\n",
    "\n",
    "assert n_classes == len(volume.metadata.labels), f\"{n_classes=} {len(volume.metadata.labels)=}\"\n",
    "\n",
    "# probas\n",
    "crop_probas = modelout.reshape(crop_probas_target_shape).astype(args.probabilities_dtype)\n",
    "\n",
    "logger.debug(f\"{crop_probas.shape=}\")\n",
    "logger.debug(f\"{crop_probas.dtype=}\")\n",
    "\n",
    "# preds\n",
    "crop_preds = crop_probas.argmax(axis=-1).astype(np.int8)\n",
    "\n",
    "logger.debug(f\"{crop_preds.shape=}\")\n",
    "logger.debug(f\"{crop_preds.dtype=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.opts.debug__save_figs:\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=3, ncols=2,\n",
    "        figsize=(2 * (sz := 20), sz), \n",
    "        dpi=120,\n",
    "    )\n",
    "\n",
    "    viz_crop_data = (\n",
    "        crop_data[:, :, slice_2halfd_data_predicted_layer] \n",
    "        if args.model_type == Args.ModelType.input2halfd else \n",
    "        crop_data\n",
    "    )\n",
    "    \n",
    "    display = viz.OrthogonalSlicesPredictionDisplay(\n",
    "        volume_data=viz_crop_data,\n",
    "        volume_prediction=crop_preds,\n",
    "        n_classes=n_classes,\n",
    "        volume_name=volume.fullname + f\".debug.crop-{crop_ijk=}\",\n",
    "    ).plot(axs=axs,)\n",
    "\n",
    "    logger.info(f\"Saving figure {(figname := display.title + '.png')=}\")\n",
    "    display.fig_.savefig(\n",
    "        fname=figs_dir / figname,\n",
    "        format=\"png\",\n",
    "        metadata=display.metadata,\n",
    "    )       \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment a batch with `batch_size=n_gpus` (1 per device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Segmenting a batch with a single instance per gpu for debug.\")\n",
    "\n",
    "batch_size = max(1, n_gpus)\n",
    "logger.debug(f\"{batch_size=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirror = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirror.scope():\n",
    "    logger.info(f\"Loading model with {mirror.__class__.__name__}.\")\n",
    "    model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_coords = crops_coordinates_sequential[:batch_size]\n",
    "logger.debug(f\"{batch_coords.shape=}\")\n",
    "\n",
    "batch_slices = [\n",
    "    tuple(slice(*coords_) for coords_ in crop_coords)\n",
    "    for crop_coords in batch_coords\n",
    "]\n",
    "\n",
    "logger.debug(f\"{batch_slices=}\")\n",
    "\n",
    "batch_data = np.stack([\n",
    "    data_volume[slice_]\n",
    "    for slice_ in batch_slices\n",
    "], axis=0)\n",
    "\n",
    "logger.debug(f\"{batch_data.shape=}\")\n",
    "\n",
    "# [model] - now i call it with a first the mirror strategy to make sure it wont break\n",
    "\n",
    "batch_modelin_target_shape = tuple([batch_size] + list(modelin_target_shape[1:]))  # adjust nb. channels\n",
    "batch_probas_target_shape = tuple([batch_size] + list(crop_probas_target_shape))\n",
    "\n",
    "logger.debug(f\"{batch_modelin_target_shape=}\")\n",
    "logger.debug(f\"{batch_probas_target_shape=}\")\n",
    "\n",
    "# modelin\n",
    "modelin = batch_data.reshape(batch_modelin_target_shape) \n",
    "\n",
    "# modelout\n",
    "modelout = model.predict(\n",
    "    modelin, \n",
    "    batch_size=batch_size,\n",
    "    steps=1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{modelout.shape=}\")\n",
    "\n",
    "# probas\n",
    "batch_probas = modelout.reshape(batch_probas_target_shape).astype(args.probabilities_dtype)\n",
    "\n",
    "logger.debug(f\"{batch_probas.shape=}\")\n",
    "logger.debug(f\"{batch_probas.dtype=}\")\n",
    "\n",
    "# preds\n",
    "batch_preds = batch_probas.argmax(axis=-1).astype(np.int8)\n",
    "\n",
    "logger.debug(f\"{batch_preds.shape=}\")\n",
    "logger.debug(f\"{batch_preds.dtype=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### segment batch with `batch_size = n_gpus * max_batch_size_per_gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Segmenting a batch with as many instances per gpu as possible for debug.\")\n",
    "\n",
    "batch_size = max(1, n_gpus) * max_batch_size_per_gpu\n",
    "logger.debug(f\"{batch_size=}\")\n",
    "\n",
    "if args.opts.override_batch_size is not None:\n",
    "    batch_size = args.opts.override_batch_size\n",
    "    logger.info(f\"{args.opts.override_batch_size=} give ==> replacing the {batch_size=}\")\n",
    "\n",
    "batch_coords = crops_coordinates_sequential[:batch_size]\n",
    "batch_slices = [\n",
    "    tuple(slice(*coords_) for coords_ in crop_coords)\n",
    "    for crop_coords in batch_coords\n",
    "]\n",
    "batch_data = np.stack([data_volume[slice_] for slice_ in batch_slices], axis=0)\n",
    "\n",
    "logger.debug(f\"{batch_data.shape=}\")\n",
    "\n",
    "batch_modelin_target_shape = tuple([batch_size] + list(modelin_target_shape[1:]))  # adjust nb. channels\n",
    "batch_probas_target_shape = tuple([batch_size] + list(crop_probas_target_shape))\n",
    "\n",
    "logger.debug(f\"{batch_modelin_target_shape=}\")\n",
    "logger.debug(f\"{batch_probas_target_shape=}\")\n",
    "\n",
    "# [model]\n",
    "# modelin\n",
    "modelin = batch_data.reshape(modelin_target_shape) \n",
    "# modelout\n",
    "modelout = model.predict(\n",
    "    modelin, \n",
    "    batch_size=batch_size,\n",
    "    steps=1,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "logger.debug(f\"{modelout.shape=}\")\n",
    "\n",
    "# probas\n",
    "batch_probas = modelout.reshape(batch_probas_target_shape).astype(args.probabilities_dtype)\n",
    "\n",
    "logger.debug(f\"{batch_probas.shape=}\")\n",
    "\n",
    "# preds\n",
    "batch_preds = batch_probas.argmax(axis=-1).astype(np.int8)\n",
    "\n",
    "logger.debug(f\"{batch_preds.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.opts.debug__save_figs:\n",
    "    \n",
    "    batch_modelout = modelout\n",
    "    \n",
    "    for idx, (crop_data__, crop_preds__) in enumerate(zip(batch_data, batch_preds)):\n",
    "        \n",
    "        fig, axs = plt.subplots(\n",
    "            nrows=3, ncols=2,\n",
    "            figsize=(2 * (sz := 20), sz), \n",
    "            dpi=120,\n",
    "        )\n",
    "        \n",
    "        viz_crop_data = (\n",
    "            crop_data__[:, :, slice_2halfd_data_predicted_layer] \n",
    "            if args.model_type == Args.ModelType.input2halfd else \n",
    "            crop_data__\n",
    "        )\n",
    "        \n",
    "        display = viz.OrthogonalSlicesPredictionDisplay(\n",
    "            volume_data=viz_crop_data,\n",
    "            volume_prediction=crop_preds__,\n",
    "            n_classes=n_classes,\n",
    "            volume_name=volume.fullname + f\".debug.batch-segm.{idx=}\",\n",
    "        ).plot(axs=axs,)\n",
    "\n",
    "        logger.info(f\"Saving figure {(figname := display.title + '.png')=}\")\n",
    "        display.fig_.savefig(\n",
    "            fname=figs_dir / figname,\n",
    "            format=\"png\",\n",
    "            metadata=display.metadata,\n",
    "        )       \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_crops = crops_coordinates_sequential.shape[0] \n",
    "\n",
    "logger.debug(f\"{n_crops=}\")\n",
    "\n",
    "last_batch_size = n_crops % batch_size\n",
    "\n",
    "if n_gpus > 1:\n",
    "    assert last_batch_size % n_gpus == 0, f\"{last_batch_size=}\"\n",
    "\n",
    "logger.debug(f\"{last_batch_size=}\")\n",
    "    \n",
    "niterations = int(np.floor(crops_coordinates_sequential.shape[0] / batch_size)) \n",
    "\n",
    "logger.debug(f\"{niterations=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if args.model_type == Args.ModelType.input2halfd:\n",
    "    proba_volume_target_shape = list(volume_shape[:2]) + [volume_shape[2] - 2 * half_pad] + [n_classes]\n",
    "    \n",
    "else:\n",
    "    proba_volume_target_shape = list(volume_shape) + [n_classes]\n",
    "\n",
    "redundancies_count_target_shape = proba_volume_target_shape[:3]\n",
    "    \n",
    "logger.debug(f\"{proba_volume_target_shape=}\")\n",
    "logger.debug(f\"{redundancies_count_target_shape=}\")\n",
    "\n",
    "proba_volume = np.zeros(proba_volume_target_shape, dtype=args.probabilities_dtype)\n",
    "logger.debug(f\"{proba_volume.shape=}\")\n",
    "\n",
    "redundancies_count = np.zeros(redundancies_count_target_shape, dtype=np.int8)  # only one channel\n",
    "logger.debug(f\"{redundancies_count.shape=}\")\n",
    "\n",
    "def process_batch(batch_start_, batch_size_):\n",
    "    batch_end = batch_start_ + batch_size_\n",
    "    \n",
    "    batch_coords = crops_coordinates_sequential[batch_start_:batch_end]\n",
    "    batch_slices = [\n",
    "        tuple(slice(*coords_) for coords_ in crop_coords)\n",
    "        for crop_coords in batch_coords\n",
    "    ]\n",
    "    batch_data = np.stack([data_volume[slice_] for slice_ in batch_slices], axis=0)\n",
    "    \n",
    "    # [model] \n",
    "    batch_probas = model.predict(\n",
    "        batch_data.reshape(batch_modelin_target_shape), \n",
    "        batch_size=batch_size_,\n",
    "        steps=1,\n",
    "    ).astype(args.probabilities_dtype).reshape(batch_probas_target_shape)\n",
    "\n",
    "    for slice_, crop_proba in zip(batch_slices, batch_probas):\n",
    "        \n",
    "        if args.model_type == Args.ModelType.input2halfd:\n",
    "            # keep x and y as is, but reduce z\n",
    "            slice_ = tuple(\n",
    "                list(slice_[:2]) +\n",
    "                [slice(slice_[2].start, slice_[2].start + 1)]\n",
    "            )\n",
    "        \n",
    "        proba_volume[slice_] += crop_proba\n",
    "        redundancies_count[slice_] += np.ones(crop_proba.shape[:-1], dtype=np.int)\n",
    "        \n",
    "logger.debug(\"Predicting and summing up the crops' probabilities.\")\n",
    "for batch_idx in pbar(\n",
    "    range(niterations), \n",
    "    prefix=\"predict-and-sum-probas\", \n",
    "    max_value=niterations\n",
    "):\n",
    "    batch_start = batch_idx * batch_size\n",
    "    process_batch(batch_start, batch_size)\n",
    "\n",
    "if last_batch_size > 0:\n",
    "    logger.info(\"Segmenting the last batch\")\n",
    "    batch_start = niterations * batch_size\n",
    "    process_batch(batch_start, last_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the min and max probas are coherent with the min/max redundancy\n",
    "min_proba_sum = proba_volume.min(axis=0).min(axis=0).min(axis=0)\n",
    "max_proba_sum = proba_volume.max(axis=0).max(axis=0).max(axis=0)\n",
    "min_redundancy = np.min(redundancies_count)\n",
    "max_redundancy = np.max(redundancies_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert min_redundancy >= 1, f\"{min_redundancy=}\"\n",
    "assert np.all(min_proba_sum >= 0), f\"{min_proba_sum=}\"\n",
    "assert np.all(max_proba_sum <= max_redundancy), f\"{max_proba_sum=} {max_redundancy=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide each probability channel by the number of times it was summed (avg proba)\n",
    "logger.debug(f\"Dividing probability redundancies.\")\n",
    "for klass_idx in pbar(range(n_classes), max_value=n_classes, prefix=\"redundancies-per-class\"):\n",
    "    proba_volume[:, :, :, klass_idx] = proba_volume[:, :, :, klass_idx] / redundancies_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes it more stable so that the sum is 1\n",
    "proba_volume[:, :, :] /= proba_volume[:, :, :].sum(axis=-1, keepdims=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that proba distribs sum to 1\n",
    "min_proba = proba_volume.min(axis=0).min(axis=0).min(axis=0)\n",
    "max_proba = proba_volume.max(axis=0).max(axis=0).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(min_proba >= 0), f\"{min_proba=}\"\n",
    "assert np.all(max_proba <= 1), f\"{max_proba=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distrib_proba_sum = proba_volume.sum(axis=-1).min()\n",
    "max_distrib_proba_sum = proba_volume.sum(axis=-1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(min_distrib_proba_sum, 1, atol=.001), f\"{min_distrib_proba_sum=}\"\n",
    "assert np.isclose(max_distrib_proba_sum, 1, atol=.001), f\"{max_distrib_proba_sum=}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proba 2 pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_volume = np.empty(proba_volume.shape[:-1], dtype=\"uint8\")\n",
    "\n",
    "np.argmax(proba_volume, axis=-1, out=pred_volume)\n",
    "\n",
    "logger.debug(f\"{pred_volume.shape=}\")\n",
    "logger.debug(f\"{pred_volume.min()=}\")\n",
    "logger.debug(f\"{pred_volume.max()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.opts.debug__save_figs:\n",
    "    fig, axs = plt.subplots(\n",
    "        nrows=3, ncols=2,\n",
    "        figsize=(2 * (sz := 20), sz), \n",
    "        dpi=120,\n",
    "    )\n",
    "\n",
    "    viz_data = (\n",
    "        data_volume[:, :, half_pad:-half_pad] \n",
    "        if args.model_type == Args.ModelType.input2halfd else \n",
    "        data_volume\n",
    "    )\n",
    "\n",
    "    display = viz.OrthogonalSlicesPredictionDisplay(\n",
    "        volume_data=viz_data,\n",
    "        volume_prediction=pred_volume,\n",
    "        n_classes=n_classes,\n",
    "        volume_name=volume.fullname + f\".debug.predicted-volume.{idx=}\",\n",
    "    ).plot(axs=axs,)\n",
    "\n",
    "    logger.info(f\"Saving figure {(figname := display.title + '.png')=}\")\n",
    "    display.fig_.savefig(\n",
    "        fname=figs_dir / figname,\n",
    "        format=\"png\",\n",
    "        metadata=display.metadata,\n",
    "    )       \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f\"Writing probabilities on disk at `{estimation_volume.probabilities_path}`\")\n",
    "np.save(estimation_volume.probabilities_path, proba_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.opts.save_probas_by_class:\n",
    "    for klass_idx in volume.metadata.labels:\n",
    "        logger.debug(f\"Writing probabilities of class `{klass_idx}` on disk at `{(str_path := str(estimation_volume.get_class_probability_path(klass_idx)))=}`\")\n",
    "        file_utils.HST_write(proba_volume[:, :, :, klass_idx], str_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(f\"Writing predictions on disk at `{(str_path := str(estimation_volume.predictions_path))}`\")\n",
    "file_utils.HST_write(pred_volume, str_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-z-slice-crops-locations.png\n",
    "\n",
    "not kept, search fro `one-z-slice-crops-locations.png` in `process-3d-crops-entire-2d-slice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### debug__materialize_crops\n",
    "\n",
    "same for\n",
    "`debug__materialize_crops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_nb_name = \"process-volume-02.ipynb\"\n",
    "this_dir = os.getcwd()\n",
    "save_nb_dir = str(estimation_volume.dir)\n",
    "\n",
    "logger.warning(f\"{this_nb_name=}\")\n",
    "logger.warning(f\"{this_dir=}\")\n",
    "logger.warning(f\"{save_nb_dir=}\")\n",
    "\n",
    "command = f\"jupyter nbconvert {this_dir}/{this_nb_name} --output-dir {save_nb_dir} --to html\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
